{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicar los algoritmos en distintos archivos\n",
    "Aplicar los algoritmos de machine learnign en archivos con distinta preparacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading required package: lattice\n",
      "Loading required package: ggplot2\n"
     ]
    }
   ],
   "source": [
    "library(caret)\n",
    "#library(e1071)\n",
    "#library(RSNNS)\n",
    "#library(lattice)\n",
    "#library(ggplot2)\n",
    "#library(Rcpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cleaned = \"1_drugs_numeric_cleaned.csv\"\n",
    "smoothed = \"2_drugs_smoothed.csv\"\n",
    "discretized = \"3_drugs_discretized.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>Age</th><th scope=col>Sex</th><th scope=col>Blood_Pressure</th><th scope=col>Cholesterol</th><th scope=col>Na</th><th scope=col>K</th><th scope=col>Drug</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>23      </td><td>F       </td><td>HIGH    </td><td>HIGH    </td><td>0.792535</td><td>0.031258</td><td>drugY   </td></tr>\n",
       "\t<tr><td>47      </td><td>M       </td><td>LOW     </td><td>HIGH    </td><td>0.739309</td><td>0.056468</td><td>drugC   </td></tr>\n",
       "\t<tr><td>47      </td><td>M       </td><td>LOW     </td><td>HIGH    </td><td>0.697269</td><td>0.068944</td><td>drugC   </td></tr>\n",
       "\t<tr><td>28      </td><td>F       </td><td>NORMAL  </td><td>HIGH    </td><td>0.563682</td><td>0.072289</td><td>drugX   </td></tr>\n",
       "\t<tr><td>61      </td><td>F       </td><td>LOW     </td><td>HIGH    </td><td>0.559294</td><td>0.030998</td><td>drugY   </td></tr>\n",
       "\t<tr><td>22      </td><td>F       </td><td>NORMAL  </td><td>HIGH    </td><td>0.676901</td><td>0.078647</td><td>drugX   </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllll}\n",
       " Age & Sex & Blood\\_Pressure & Cholesterol & Na & K & Drug\\\\\n",
       "\\hline\n",
       "\t 23       & F        & HIGH     & HIGH     & 0.792535 & 0.031258 & drugY   \\\\\n",
       "\t 47       & M        & LOW      & HIGH     & 0.739309 & 0.056468 & drugC   \\\\\n",
       "\t 47       & M        & LOW      & HIGH     & 0.697269 & 0.068944 & drugC   \\\\\n",
       "\t 28       & F        & NORMAL   & HIGH     & 0.563682 & 0.072289 & drugX   \\\\\n",
       "\t 61       & F        & LOW      & HIGH     & 0.559294 & 0.030998 & drugY   \\\\\n",
       "\t 22       & F        & NORMAL   & HIGH     & 0.676901 & 0.078647 & drugX   \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "Age | Sex | Blood_Pressure | Cholesterol | Na | K | Drug | \n",
       "|---|---|---|---|---|---|\n",
       "| 23       | F        | HIGH     | HIGH     | 0.792535 | 0.031258 | drugY    | \n",
       "| 47       | M        | LOW      | HIGH     | 0.739309 | 0.056468 | drugC    | \n",
       "| 47       | M        | LOW      | HIGH     | 0.697269 | 0.068944 | drugC    | \n",
       "| 28       | F        | NORMAL   | HIGH     | 0.563682 | 0.072289 | drugX    | \n",
       "| 61       | F        | LOW      | HIGH     | 0.559294 | 0.030998 | drugY    | \n",
       "| 22       | F        | NORMAL   | HIGH     | 0.676901 | 0.078647 | drugX    | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "  Age Sex Blood_Pressure Cholesterol Na       K        Drug \n",
       "1 23  F   HIGH           HIGH        0.792535 0.031258 drugY\n",
       "2 47  M   LOW            HIGH        0.739309 0.056468 drugC\n",
       "3 47  M   LOW            HIGH        0.697269 0.068944 drugC\n",
       "4 28  F   NORMAL         HIGH        0.563682 0.072289 drugX\n",
       "5 61  F   LOW            HIGH        0.559294 0.030998 drugY\n",
       "6 22  F   NORMAL         HIGH        0.676901 0.078647 drugX"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nc_dat = read.csv(numeric_cleaned,  header = TRUE)\n",
    "sm_dat = read.csv(smoothed,  header = TRUE)\n",
    "dc_dat = read.csv(discretized,  header = TRUE)\n",
    "head(nc_dat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividiendo los datos\n",
    "Dividiendo los datos para entrenamiento y pruebas usando caret."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "drugA drugB drugC drugX drugY \n",
       "   22    16    16    53    91 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "drugA drugB drugC drugX drugY \n",
       "   17    12    12    40    69 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "drugA drugB drugC drugX drugY \n",
       "    5     4     4    13    22 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(300)\n",
    "nc_indxTrain <- createDataPartition(y = nc_dat$Drug, p = 0.75, list = FALSE)\n",
    "nc_training <- nc_dat[nc_indxTrain,] # devuelve las filas en nc_indxTrain (similar a un roi)\n",
    "nc_testing <- nc_dat[-nc_indxTrain,] # devuelve todas las filas que no esten en nc_indxTrain\n",
    "\n",
    "sm_indxTrain <- createDataPartition(y = sm_dat$Drug, p = 0.75, list = FALSE)\n",
    "sm_training <- sm_dat[nc_indxTrain,] # devuelve las filas en nc_indxTrain (similar a un roi)\n",
    "sm_testing <- sm_dat[-nc_indxTrain,] # devuelve todas las filas que no esten en nc_indxTrain\n",
    "\n",
    "dc_indxTrain <- createDataPartition(y = dc_dat$Drug, p = 0.75, list = FALSE)\n",
    "dc_training <- dc_dat[dc_indxTrain,] # devuelve las filas en nc_indxTrain (similar a un roi)\n",
    "dc_testing <- dc_dat[-dc_indxTrain,] # devuelve todas las filas que no esten en nc_indxTrain\n",
    "\n",
    "table(nc_dat$Drug)\n",
    "table(nc_training$Drug)\n",
    "table(nc_testing$Drug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando kNN a \"numeric cleaned\"\n",
    "kNN necesita que las datos esten normalizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Created from 150 samples and 6 variables\n",
       "\n",
       "Pre-processing:\n",
       "  - centered (3)\n",
       "  - ignored (3)\n",
       "  - scaled (3)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols <- names(nc_training)\n",
    "nc_trainX <- nc_training[, cols != \"Drug\"] # selecciona tdoso los datos de entrenamiento exceptuando la columna Drug\n",
    "nc_preProcValue <- preProcess(x = nc_trainX, method = c(\"center\", \"scale\"), tuneLength = 20)\n",
    "nc_preProcValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenando el sistema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k-Nearest Neighbors \n",
       "\n",
       "150 samples\n",
       "  6 predictor\n",
       "  5 classes: 'drugA', 'drugB', 'drugC', 'drugX', 'drugY' \n",
       "\n",
       "Pre-processing: centered (7), scaled (7) \n",
       "Resampling: Cross-Validated (10 fold, repeated 3 times) \n",
       "Summary of sample sizes: 135, 135, 136, 135, 136, 133, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  k  Accuracy   Kappa    \n",
       "  5  0.7570343  0.6457641\n",
       "  7  0.7797958  0.6717484\n",
       "  9  0.8097759  0.7146852\n",
       "\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final value used for the model was k = 9."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(500)\n",
    "ctrl <- trainControl(method = \"repeatedcv\", repeats = 3) # repeatedcv is repeated cross validation\n",
    "knnFit <- train(Drug ~ ., data = nc_training, method = \"knn\", trControl = ctrl, preProcess = c(\"center\", \"scale\"))\n",
    "knnFit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "graficando **number of neighbors** vs **accuracy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAIAAAByhViMAAAACXBIWXMAABJ0AAASdAHeZh94\nAAAgAElEQVR4nOzdeWBU5d328WuykgTDTsCyCIpoq+IuIOKuJYGwKggSlqTaWpfuq13sU2tb\nuzx0eWsroCIJCbsgoLIoWwIK6FMVVBAEZTGEgCQkZJk57x+pMc0+k5m5zznz/fwVZsbhkgFy\ncZ/7dx+PZVkCAACA80WZDgAAAIDgoNgBAAC4BMUOAADAJSh2AAAALkGxAwAAcAmKHQAAgEtQ\n7AAAAFyCYgcAAOASFDsAAACXoNgBAAC4BMUOAADAJSh2AAAALkGxAwAAcAmKHQAAgEtQ7AAA\nAFyCYgcAAOASFDsAAACXoNgBAAC4BMUOAADAJSh2AAAALkGxAwAAcAmKHQAAgEtQ7AAAAFyC\nYgcAAOASFDsAAACXoNgBAAC4BMUOAADAJSh2AAAALkGxAwAAcAmKHQAAgEtQ7AAAAFyCYgcA\nAOASFDsAAACXoNgBAAC4BMUOAADAJSh2AAAALkGxAwAAcAmKHQAAgEtQ7AAAAFyCYgcAAOAS\nFDsAAACXoNgBAAC4BMUOAADAJSh2AAAALkGxAwAAcAmKHQAAgEtQ7AAAAFyCYmdHO3fufPTR\nR02nQBB8/PHH9913n+kUCIKysrKMjIyysjLTQRAE991338cff2w6BYLg0Ucf3blzp+kU9kKx\ns6OdO3cuWrTIdAoEwQcffDB37lzTKRAEhYWFzz//fGFhoekgCIK5c+d+8MEHplMgCBYtWkSx\nq4diBwAA4BIUOwAAAJeg2AEAALgExQ4AAMAlKHYAAAAuQbEDAABwCYodAACAS8SYDmBHXq93\n06ZNXq/XVIA9e/aUlZWtW7fOVAAEy5tvvmlZFh+lC3z66aeStm7dum/fPtNZ0FaWZe3atcuy\nLNNB0FZlZWV79uwx+HdsdHT08OHDo6OjTQVoyMPv7IbWr19/2223mU4BAADsbu3atbbqDKzY\nNaK6ujo+Pv7s2bOmApSVlZ05c6Zbt26mAiBYKioqiouLe/bsaToI2srr9RYWFnbv3t1W/zRH\nYI4ePdq5c+f4+HjTQdBWx48fT0pKSkxMNBUgISGhurra1M/eKPbYAQAAuATFDgAAwCUodgAA\nAC5BsQMAAHAJih0AAIBLUOwAAABcgmIHAADgEhQ7AAAAl6DYAQAAuATFDgAAwCUodgAAAC5B\nsQMAAHAJih0AAIBLUOwAAABcgmIHAADgEhQ7AAAAl6DYAQAAuATFDgAAwCUodgAAAC5BsQMA\nAHAJih0AAIBLUOwAAABcgmIHAADgEhQ7AAAAl6DYAQAAuATFDgAAwCUodgAAAC5BsQMAAHAJ\nih0AAIBLUOwAAABcgmIHAADgEhQ7AAAAl6DYAQAAuESM6QBf8Hg8NV9YltWWl3k8nubfAQAA\nwJXsUuzqtrFmmlkrXwYAABCBnHQptl6TsyyrdvWu7mvCGwoAAMAubLFi12hjC2A1rua/otsB\nAOBuXktbDumNAwkpydE39Nd5HU0Hsg1bFLtWqlf4GpY/v7qg1+s9ffp0o0+VlpZK8vl8gSZt\nq5r/EYMBECx8lK5R8yH6fD7+6egOlmXxB9O53jii+1+M2n1c/ToknKyIOrlKD1xt/fY2Kz7a\nQJjS0tKTJ082+lRycnJ0dLgz2WKbWsOK1vw2u5ov2jg8MW3atHnz5jX1bExMzKFDh1p8EwAA\nEE6HS6NvXdztjr5nHxt6ulO8T9Kmw/HferXDLX0q/jD8szCH6devX0pKSlOFISMj47nnngtz\nJIcVu9YMT7Sy2J04ceKjjz5q9KmCgoLvfve7Net2Rpw9e7a8vLxTp06mAiBYKisrT58+3bVr\nV9NB0FY+n6+4uLhz585RUU7amoxGFRUVJScnx8XFmQ6CQHxvXXT+x56t06s9Hp08eTIhIaFd\nu3YbD3ruzInZ92B1r3PC2mrOOeecJ598cujQoY0+e95553Xp0iWceeSsS7HB2opXo0uXLk39\nchcVFXk8ntjY2MDeue2qqqrMBkCw1Fzr4aN0Aa/XKykmJib8F1YQCjExMfzBdKhthzXuy4qL\ni5X0ysH4oefF9D8n9tbz1amddhyL6dc5rGE8Hs+AAQOuuuqqsP6szeKfngAAwDEqqpUYK0nz\n/63pazrsP+mR5PEoMVZnqw1nswNbrNi1OBUBAAAg6eJuKvhYg1KUtUKPXV86tFeUpEOf6XCJ\nLmbbiyNW7OpOS9SdR6P/AQAQae67Sot2a2SOHrpO911WLqmkUve/qCt66KqepsPZgC1W7PTf\npa2ZutbKlwEAAFca2EXJ8TpdoZ1H9URFUok37sV9OidOq6eIw4hkqxU763MNH2/Ny5p6PQAA\ncIfTFUrN1qXdteM+XdZduz6NOXnW85Mb9PYDuiC8YxO2ZZcVOwAAgGZU+TRhoSq9Wj5JnRN0\nxVd1/PhnSUlJiYmJpqPZCMUOAADYnWXpayv0TqEKstQ5wXQaG6PYAQAAu/vpBi3do40z1LeD\n6Sj2RrEDAAC29vQu/SFfKyfrih6mo9gexQ4AANjX6r16YJX+OVJ3nm86ihPYaCoWAACgrh1H\nNHGxHh2umVeYjuIQFDsAAGBHB05pZI5GD9TPh5uO4hwUOwAAYDsnyjVivr7SXXNHc/KwHyh2\nAADAXsqrNXqB4qK1dKLiok2ncRSGJwAAgI34LE1dqoOfqSBTHeJNp3Eaih0AALCR77ysdfu1\neaZ6JZuO4kAUOwAAYBd/yNc/dmj1FF3a3XQUZ6LYAQAAW1i0Wz9ap2fH6NZ+pqM4FsMTAADA\nvM2HlLFMv7td915mOoqTUewAAIBhe4o0JlfTL9d3h5iO4nAUOwAAYNLRUqVma2hv/S3VdBTn\no9gBAABjSiqVlq3uScqdoGgOIm4zhicAAIAZVT7dvUinK5SfqaRY02lcgWIHAADMeGSNdhxR\nfqa6J5mO4hYUOwAAYMAvX9Ozb2n9NA3obDqKi1DsAABAuGW/rcc3a9FdGtLLdBR3YXgCAACE\n1YYDmvmC/nynxlxkOorrUOwAAED4vF2ocXn63lA9eK3pKG5EsQMAAGHyyWmlZmvEAP36ZtNR\nXIpiBwAAwuGzCqVm64LOenaMPBxZFxoUOwAAEHKVXo3Pk8/SsomKjzadxr2YigUAAKFlWcpc\noT1FKshUx3am07gaxQ4AAITWD9ZpxfvaNEN9OpiO4nYUOwAAEEL/3KlZ27RqigalmI4SASh2\nAAAgVFZ+oAdXa3a6bu9vOkpkYHgCAACExBtHdM9iPXaTpg0yHSViUOwAAEDwfXhSI3N0z6X6\nyQ2mo0QSih0AAAiyojKNmK+rz9U/0kxHiTAUOwAAEEzl1UpfoOR45U1QDEUjvBieAAAAQeO1\nNHmJjpWqIEvt40yniTwUOwAAEDTfekmbD2prplKSTEeJSBQ7AAAQHE9s0exdWpehgV1MR4lU\nFDsAABAEue/o568qd4Ku7206SgRjTyMAAGirjQc1fbn+cIfGX2w6SmSj2AEAgDZ597jG5urB\na/XIdaajRDyKHQAACNyREqVm67b++v3tpqOAYgcAAAJ2ukJpOerbQfPGKspjOg0YngAAAIGp\n8mnCQlVUa32G2lEo7IHPAQAA+M2y9LUVeqdQ+ZnqnGA6DT5HsQMAAH579FUt3aPXpuu8jqaj\noA6KHQAA8M/sXfrdFi2dqCt7mo6C/8bwBAAA8MPqvfrGKv01VekDTUdBAxQ7AADQWjuPauJi\n/fQGfeNq01HQGIodAABolQOnlJat0QP1ixtNR0ETKHYAAKBlJ8o1Yr6+3E1zR8vDkXV2RbED\nAAAtOFut0QsUF61lkxQXbToNmsZULAAAaI7P0r1L9dEpbctSh3jTadAsih0AAGjOd17Wuv3a\nNEO9kk1HQUsodgAAoEl/LNA/dmj1FF2WYjoKWoFiBwAAGrdot364Vs+M0a39TEdB6zA8AQAA\nGrH5kDKW6be3aeplpqOg1Sh2AACgvj1FGpOraYP0vaGmo8AfFDsAAPBfjpYqNVtDe+vvaaaj\nwE8UOwAA8IWyKo3NVbdE5U5QNAcROw3DEwAA4D+8lu5ZoqIy5WcqKdZ0GviPYgcAAP7jodXK\n/1hbZ6p7kukoCAjFDgAASNJjG/XsW1o/TRd2MR0FgaLYAQAAZb+tX2/Swrs0pJfpKGgDhicA\nAIh0r36kmS/oz3dq7EWmo6BtKHYAAES0tws1NlffHaIHrzUdBW1GsQMAIHIdLlFqtkYM0OO3\nmI6CYKDYAQAQoT6rUGq2zu+kZ8fIw5F1rkCxAwAgElV6NT5P1T4tm6T4aNNpECRMxQIAEHEs\nS1krtKdIBZnq1M50GgQPxQ4AgIjzg3Va/p42zVCfDqajIKgodgAARJZ/7tSsbXpxsi7vYToK\ngo1iBwBABHnxAz24WrPTdcf5pqMgBBieAAAgUrxxRJMW65c3adog01EQGhQ7AAAiwocnNTJH\n91yqn95gOgpChmIHAID7FZUpNVtXn6t/pJmOglCi2AEA4HLl1UpfoHPilDdBMXzndzWGJwAA\ncDOvpSlLdKxU+ZlqH2c6DUKMYgcAgJt96yVtOqitmerR3nQUhB7FDgAA13pii2bv0tqpGtjF\ndBSEBcUOAAB3yn1HP39VuRM0rI/pKAgXtlACAOBCGw9q+nI9ebvGX2w6CsKIYgcAgNvsPq6x\nufrmtfrWYNNREF4UOwAAXOVIiUZk67b+evJ201EQdhQ7AADco6RSaTnq20HzxirKYzoNwo7h\nCQAAXKLKpwkLVVGt9Rlqx3f4iMTHDgCAG1iWvrZCbx5VfqY6J5hOA0ModgAAuMHPXtWi3dow\nTRd0Nh0F5lDsAABwvNm79NstWjpR133JdBQYxfAEAADOtnqvvrFKfxmh9IGmo8A0ih0AAA62\n86gmLtZPbtAD15iOAhug2AEA4FQHTmlkjtIH6pc3mo4Ce6DYAQDgSCfKNWK+Lu6qZ0bLw5F1\nkESxAwDAic5Wa0yuYqO1dKLiok2ngW0wFQsAgMP4LN27VAdOqiBLHduZTgM7odgBAOAw331F\na/dr8wz1TjYdBTZDsQMAwEn+VKD/94ZWT9FlKaajwH4odgAAOMai3frBWj0zRrf2Mx0FtsTw\nBAAAzrD5kDKW6YnbNPUy01FgVxQ7AAAcYF+xxudp2iB9f6jpKLAxih0AAHZ3vEwjsnVdL/09\nzXQU2BvFDgAAWyur0qgcdWqn3AmK5iBiNIvhCQAA7Mtr6Z4lOl6mgkwlxZpOA9uj2AEAYF8P\nr1H+x9o6U92TTEeBE9io2Hk+v9GdZVmBvayV7wAAgCP8aqOeeVPrp+nCLqajwCHsUuw8Hk9t\nG6v7detf1sp3AADAEXLe1v9s0sK7NKSX6ShwDicNT9Sra5Zl1S7RAQDgJq9+pBkv6E93auxF\npqPAUWxR7Nre2Oh8AADXeKdQ4/L0nSF66FrTUeA0drkU2xo1dS1Y11urqqpKS0sbfarmcZ/P\nF/Cbt1HN/5fBAAgWPkrXqPkQfT4f/2h0B8uybPsH83CJUrOj7uhv/c9Nll0z2ojxj7K0tPTk\nyZONPtW+ffvY2HBPMttiL1rDitb8NruaL5raYNfiO9TIyMh4/vnnm3o2Jibm0KFDLSYHACCI\nSio9Y1Z06RhvLUgrjosy/w0azevXr19KSkpThWHq1Knz5s0LcySHFbumVuwCKHanT58uKipq\n9KlNmzbdf//9ZWVlrf9fCK7y8vLy8vLOnTubCoBgqaysPHXqVPfu3U0HQVt5vd4TJ0506dIl\nOjradBa0VWFhYceOHePi4kwHqa/Sq5ELoo6d8Wyc5u3UznQaJyguLk5ISEhISDAVICkp6amn\nnho+fHijz3bt2jU5OTnMkZx0KbbRjXQBF9Pk5OSmfrn37t3r8XgM/vUdFRUlie8fLsBH6TLR\n0dF8mu4QFRVlt4/SsnT/Cr13QvmZ6ppkr2x2Zvaj9Hg8PXr06N+/v6kADTmp2AEA4FY/XKfl\n72nTDPXtYDoKnMwWU7H1hlgDWIdr+zsAAGDKP3fqf7dp8d26vIfpKHA4WxS75tWdlqC9AQBc\n5sUP9OBq/WuU7jjfdBQ4n10uxdYtbc3UtWZe1sp3AADAPt44okmL9YsbNf1y01HgCnYpdmq6\njTUscP6+AwAANrT/pEbmaNIlerTxqUrAbw64FAsAgPsUlWlEtq7qqadGmo4CF6HYAQAQbuXV\nSl+gc+K08C7F8K0YwWOjS7EAAEQCr6UpS3SsVPmZam+7Y5LhbBQ7AADC6tsvaeNBbZ2pHu1N\nR4HrUOwAAAif327R07u0dqou6mo6CtyIYgcAQJjkvaufvaoF4zWsj+kocCl2bAIAEA4bD2ra\nMj15uyZ82XQUuBfFDgCAkNt9XGNz9cA1+tZg01HgahQ7AABC60iJUrN1Q189eYfpKHA7ih0A\nACFUUqm0HPVorwXjFe1p+fVAW1DsAAAIlSqfJixUaaVWTlZirOk0iABMxQIAEBKWpftW6s2j\nys9Ut0TTaRAZKHYAAITEz1/Twne1PkMXdDYdBRGDYgcAQPDNeVNPbNaSiRrcy3QURBL22AEA\nEGRr9unrL+ovIzR6oOkoiDAUOwAAgmnnUd29SD8epgeuMR0FkYdiBwBA0Bw4pZE5Sh+ox24y\nHQURiWIHAEBwFJcrNVsXd9Uzo+XhyDqYQLEDACAIzlZrdK5iorR0ouKiTadBpGIqFgCAtvJZ\nmrpMB06qIEsd25lOgwhGsQMAoK2++4pe+VCbZ6h3sukoiGwUOwAA2uTP2/T317V6ii5LMR0F\nEY9iBwBA4Fa8rx+s1dzRuq2/6SgAwxMAAARs+2Hds0SP36Kpl5mOAkjya8XO0+zotmVZbQ4D\nAIBj7CvWqBzde5l+cL3pKMDnWlXsmq90dV9DvQMARILjZRqRrWu/pL+nmo4C1NFysattdc2X\ntpqXeTweuh0AwN3KqpS+QJ3aKe8uxbCnCXbSQrFr/TpczWs8Hg/dDgDgYl5Lk5eo8IzyM5UU\nazoN8N9aKHb+VjQqHQDA3R5eo60fa+tMpSSZjgI0wHEnAAC01v9s0jNval2GLuxiOgrQGP+K\nXTNTFKzVAQDcLedt/Wqj8iZoaG/TUYAm+LHnszWzsQAAuNKrH2nGC/rTnRp3sekoQNP8vhTL\nyhwAINK8U6hxefr2YD10rekoQLOY0gYAoDmHS5SarTvP129uNR0FaAnFDgCAJp2uUGq2+nXS\nc2MVxY4k2J4fxa72pLqQhQEAwEaqfBq/UNU+LZ+k+GjTaYBWCOResY12O/beAQDcxLKU+YJ2\nH1d+pjq1M50GaB3OsQMAoBE/Wq/l72nTDPXtYDoK0Gp+FDvW5AAAEeJfO/XnAq2crMt7mI4C\n+IMVOwAA/suqvfrmaj09SneebzoK4KdAil3dPXYs4wEA3OSNI5q4SD+/UdMvNx0F8F9bbylW\n8wj1DgDgAvtPalSOJl2inw03HQUISCBTsXVrXM2DHo+HbgcAcLSiMo3I1pU99dRI01GAQPl9\nQHG9AkefAwC4QHm1RueqfZwW3qUYDu+HYzE8AQCIdD5L9y7VkRIVZKp9nOk0QBtQ7AAAke5b\nL+m1j7R1pnq0Nx0FaBu/i1297XTcYQwA4Gi/26qnd2ntVF3U1XQUoM38O6C4dlSi4VPBDAUA\nQFjkvaufrtdzYzWsj+koQDD4t2JXU+A4xw4A4AKbDmraMj15h6ZcajoKECSB7LGjzAEAnG73\ncY3J1cwr9O3BpqMAwcNINwAg4hwt9aRm64a++muq6ShAULWwYlf3xhLNz0mwjAcAcITSKs9d\nL8SmtNeC8YpmAhDuwnEnAIAIUuXTfWs7lVZqbYYSY02nAYKthWJXdx2ONTkAgKNZlu5bqbeL\nYjdNq+qWyEnEcCE/9th5PJ5Gr8Y29TgAALbyi9e08F09e2fx+Z1YqoA7cSkWABAR5ryp32zW\nkom6KrnKdBYgVFpesfN8rt4P6z0OAIBtrdmnr7+oWSM0eqDpKEAoBee4E7bfAQBsa9dR3b1I\nPxqmb15jOgoQYi1fiq0tbXWPPgEAwBE+OqW0HI26UL+6yXQUIPQ4oBgA4FrF5RqRrYu66pkx\nYusQIoEfwxOs1QEAHORstUbnKiZKyyYqPtp0GiAsgjAVyyVaAIDdWJayVujASRVkqWM702mA\ncPGv2DEDCwBwhO++opUfaPMM9U42HQUIIz+KXTOtjuU6AIB9/GOH/va6Vk/RZSmmowDh5fel\nWMuy6l17ZRkPAGAfK97Xw2s0d7Ru6286ChB2/k3F1pS5eutztVUPAACzXj+se5bo17do6mWm\nowAmcNwJAMAl9hVrZI7uvUw/vN50FMAQ/4pd3ZW5ujcZC2YiAAD8d7xMI7J17Zf091TTUQBz\n/F6xq9ftaHUAAOPKqpS+QB3bKe8uxXAtChHMvwOKa2tcvX11TMUCAEzxWpq8RIVnlJ+ppFjT\naQCj/JuKrVvgKHMAADt4ZI22HFJ+plKSTEcBTAvCnScAADDl15s0902ty9CFXUxHAWyghWLX\n+i10LOABAMJswTt6bKPyJmhob9NRAHtgiykAwJFe+0gzluuPd2jcxaajALbRwopdvXW4evec\naPQRAABC7Z1Cjc3TI4P18HWmowB24seKXaMdruaHHHoCAAibwyVKzdad5+uJW01HAWyGS7EA\nACc5XaG0bPXrpOfGKopVBeC/UewAAI5R5dP4haryafkkxUebTgPYj9/HnXg8noZ77AAACDXL\nUtYKvVuogix1amc6DWBLgdx5omGZY3gCABBqP16vZXu0cYb6djAdBbCrQO48wc3EAABh9q+d\n+lOBVk7WFT1MRwFsLJA7T1DmAADhtGqvvrla/xqlO883HQWwN4YnAAC2tuOIJi7Sz2/UjMtN\nRwFsr1W3FGvNYXUs4wEAgm7/SY3M0aRL9LPhpqMATsCKHQDApk6UKzVbV/bUUyNNRwEcwo9b\nirEmBwAIm/JqpS9QUpwW3qUYViGA1glkeAIAgJDyWbp3qY6UqCBT7eNMpwGco1V77FqD9TwA\nQLB8+2Wt36/NM9WjvekogKOwYgcAsJffb9VTO7Rmii7tbjoK4DR+7LEDACDUFr6rn6zXc2N1\nSz/TUQAHYj8qAMAuNh1UxjI9eYemXGo6CuBMQSh2Ho+n9VvxAABo1O7jGpOrmVfo24NNRwEc\ny789dhQ4AEAoHC1VaraG9dFfU01HAZzMjxW7ZlodW/EAAAErqVRatlLaK3eCollAANrA76lY\ny7Lq3mdMLOMBANqgyqe7Fup0hQqylBhrOg3gcP4Vu5oyV9vtah/0eDws2gEA/GVZun+ldh3V\n1kx1SzSdBnA+zrEDABjzy43Ke1frMzSgs+kogCv4NxVbd6Gu9msuxQIAAjDnTT2+STnjNbiX\n6SiAW/h93Em9bkerAwAEYM0+ff1FzRqh0QNNRwFcxI9iV3cXXb0ddWywAwC03q6junuRfjRM\n37zGdBTAXQIZnmj4NQAArfTRKaXlaNSF+tVNpqMArsMtxQAA4fNZhdIX6KKuemaM2MsDBF0L\nxY5ddACAYKn0alyefJaWTVR8tOk0gBu16lJsbbcL6eXX5n+Wpvpl3ReHJycAIACWpZkv6P0i\nFWSpYzvTaQCXaqHY1TSkhiebBL051T3iuNHjjltsey2+AwDAoO+t1coPtGmGeiebjgK4V6tW\n7BrePcwOa2PNNDluhgEAtvLUDv11u1ZN0aAU01EAVwtwKrZew2tjhQqgltHbAMApVn6gh9Zo\nTrpu7286CuB2Ad5SrF7Dc2LNKi8vP3r0aKNP1Tzu9XrDm+gLPp/PbAAECx+la9R8iHyUAXjj\niGfS4qjHbvRNucSyz6+fz+fj03QHsx+lZVnHjh3bv39/o8/27NkzISEhzJHaWsiCck22YS9s\nvim25vUtds2MjIznn3++qWdjYmIOHTrUQm4AQLM+Oh2d/kLXO/uefXL4Z6azAMHXr1+/Hj16\nHDx4sNFnp06dOm/evDBHCrDY1ZtRDe6l2EYfafHZelvuWkxVWlpaWFjY6FObNm26//77y8rK\nWhM+FMrLy8vLyzt35p7YjldZWXnq1Knu3bubDoK28nq9J06c6NKlS3Q0p3S0VlGZbng2ekBn\na+ndvhg7nZpaWFjYsWPHuLg400HQVsXFxQkJCeFfFauVlJT01FNPDR8+vNFnu3fv3r59+zBH\n8u9SbMMzR8J/BbapzlezM6/h101p3759U7/ce/fu9Xg8Bv/6joqKksT3Dxfgo3SZ6OhoPs1W\nKqvSmIXq0E4L7/bEx9ruFy0qKoqP0h3MfpQej6dHjx79+9to96h/59jVsueOOnumAoBI47U0\neYkKzyg/U0mxptMAkaSFYhfcS65NqTcGG8AohhOnNwDArR5Zoy2HtDVTKUmmowARxr9z7IwI\noPPR8wDAlMc3a+6bWpehgV1MRwEiT6vuPBEG9XbIteU/D+wdAABtt+Ad/fI15U3Q0N6mowAR\nKcBz7EKxJNbUG9Y7u9jf/xwAEB6vfaQZy/XHOzTuYtNRgEhlpwF0AIBjvXtcY/P08HV6+DrT\nUYAIRrEDALTVkRKNmK87ztdvbzMdBYhsFDsAQJucrlBqtvp10ryximrhCFEAoUWxAwAErsqn\nCQtV5dPySYrnxF/AtACHJ5hUAABYlrJW6J1CFWSpUzvTaQAEdkuxmlbXltNJAAAu8JMNWrZH\nG2eobwfTUQBI8qvY1T0ort7XdDsAiDRP79If87Vysq7oYToKgM/5vceu3qly9VbvAACRYNVe\nPbBK/xylO883HQVAHQxPAAD8s+OIJi7Sz4ZrxuWmowD4bxQ7AIAfDpzSyBxNukQ/v9F0FAAN\n+FHsaq+6NjpCAQBwvRPlGjFfV/TUUyNNRwHQGP9W7FpzO1cAgCuVVyt9geKilTtBMVzvAWzJ\n73Ps6nU4Kh0ARAKfpXuX6tBnKshUh3jTaQA0IcADigEAEeXbL2v9fm2eqV7JpqMAaBoHFAMA\nWvBkvp7aoTVTdGl301EANMuPXRLNHFAczEQAADtZ+K5+vE5z0nVLP9NRALSEA4e+mSIAACAA\nSURBVIoBAE3adFDTluv3t+vey0xHAdAKzDUBABq3p0hjcjX9cn1niOkoAFqHYgcAaMTRUo2Y\nr2F99LdU01EAtBoHFAMA6iupVFq2Utord4Ki+WsecA7/pmIty2q0yTEYCwCuUeXTXQt1ukL5\nmUqMNZ0GgD84oBgA8AXL0v0rtfOo8jPVPcl0GgB+4oBiAMAXHtuo3He0YZoGdDYdBYD/Ail2\nda/GsmIHAK4x9039epMW363BvUxHARCQQO480fAR6h0AON1L+3T/i5o1QmMuMh0FQKD8KHaN\n3kOs5kGPx0O3AwDnertQkxbrh9frm9eYjgKgDdp054mGPwQAOM4np5WarRED9D83m44CoG04\noBgAItpnFUrN1gWd9ewYcTIp4HQUOwCIXJVejc+Tz9KyiYqPNp0GQJv5PRVbbzsdd54AAIey\nLGWu0J4iFWSqYzvTaQAEgx/Frva2Ew3LHDvtAMBxvr9WK97Xphnq08F0FABB4vctxcQ5dgDg\nfE/t0F+2a9UUDUoxHQVA8ARyQDFlDgAcbeUHemiN5qTr9v6mowAIKj+GJzweDzvqAMDpXj+s\nSYv1q5uVMch0FADBxlQsAESQD09q1AJNuVQ/HmY6CoAQ8KPYWZZVOz8BAHCcojKNmK9rztX/\nSzMdBUBoBHJLsUa7HRvvAMDOyquVvkAd2invLsVwtQZwqUCGJwAAzuK1NHmJjpWqIEtJsabT\nAAgZ/86xC10OAEDofOslbT6orZlKSTIdBUAosWIHAC73m82as0trMzSwi+koAEKsVcWuZlNd\nwxW7ph4HANhE7jv6xWvKm6Dre5uOAiD0Wt5A2+IYLHOyAGBPr32k6cv1hzs07mLTUQCERWsn\noxpdlmOtDgBs693jGpunh67TI9eZjgIgXFoodi1ebG1491gAgHFHSpSarTvO1+9uMx0FQBhx\nlhEAuM3pCqVmq28HPTdGUfy7G4gkFDsAcJUqnyYsVKVXyyepHScfABGGP/QA4B6Wpa+t0DuF\nKshS5wTTaQCEHcUOANzjpxu0dI82zlDfDqajADChhUuxLc5GcJQdANjE07v0h3wtultX9DAd\nBYAhrV2xa1jgmIQFAPtYvVcPrNI/R+rO801HAWBOy8XOsqzaDtdomWO5DgDM2nFEExfr0eGa\neYXpKACMatWKXVMXZKl0AGDcgVMamaPRA/Xz4aajADDNj+EJahwA2M2Jco2Yr69019zRYoMM\nAM6xAwCnKq/W6AWKi9bSiYqLNp0GgA1w3AkAOJLP0tSlOviZCjLVId50GgD2QLEDAEf6zsta\nt1+bZ6pXsukoAGyDYgcAzvOHfP1jh1ZP0aXdTUcBYCcUOwBwmEW79aN1enaMbu1nOgoAm2F4\nAgCcZPMhZSzT727XvZeZjgLAflpYsWv97SU4DAUAQm1Pkcbkavrl+u4Q01EA2BIrdgDgDEdL\nlZqtob31t1TTUQDYVQsrdvXW4Zq6YyzLdQAQUiWVSstW9yTlTlA0BxEDaIIfwxONdriaO8l6\nPB66HQCESJVPdy/S6QrlZyop1nQaADbGVCwA2N0ja7TjiPIz1T3JdBQA9kaxAwBb++VrevYt\nrZ+mAZ1NRwFge34PT9Sbk2392CwAwF/Zb+vxzcoZryG9TEcB4AR+rNjVbKdTY2WODXYAEHQb\nDmjmC/rznRpzkekoABzCv0uxNQWubrGj0gFAKLxdqHF5+t5QPXit6SgAnCOQPXaUOQAIqU9O\nKzVbIwbo1zebjgLAUTigGADs5bMKpWbrgs56dozYxgzAL36v2NVeh617WZY1PAAIikqvxufJ\nZ2nZRMVHm04DwGn8K3bMwAJA6FiWMldoT5EKMtWxnek0ABzIj0uxtYtzddfnGo5TAAAC8/21\nWvG+Vk9Rnw6mowBwJg4oBgBbeGqH/rJdq6ZoUIrpKAAci2IHAOat/EAPrdHsdN3e33QUAE7W\n1qlYLsICQBu9flj3LNZjN2naINNRADic33eeqG1yHFMMAG334UmNWqB7LtVPbjAdBYDz+bdi\n12iBo9UBQGCKyjRivq4+V/9IMx0FgCv4vceOGgcAQVFerfQFSo5X3gTFcFo8gGDwo9g1dRYx\nZxQDgL+8liYv0bFSFWSpfZzpNADcgqlYADDgWy9p80FtzVRKkukoAFyk5WJXb+6VMVgAaKPf\nbNbsXVqXoYFdTEcB4C7BWbHjOiwAtFLuO/rFa8qdoOt7m44CwHVaLna1pY29dADQRq99pOnL\n9Yc7NP5i01EAuBGDWAAQJu8e19g8PXitHrnOdBQALuXfAcWhywEA7nakRKnZur2/fn+76SgA\n3CsIK3Z1b0cBAGjodIVSs9W3g+aNVRR/XwIIGf+GJyhwAOCvKp8mLFSlV8snqR1nTAEIJb8P\nKG4UV2kBoFGWpa+t0DuFys9U5wTTaQC4XSC3FKs3HssyHgA05dFXtXSPXpuu8zqajgIgAvi3\nx66mzNVbn6utegCAumbv0u+2aP44XdnTdBQAkYHjTgAgJFbv1TdW6a+pSh9oOgqAiOFfsau7\nMlf7Nct1AFDPzqOauFg/vUHfuNp0FACRxO8Vu3rdjlYHAPUcOKW0bI0eqF/caDoKgAjjR7Gr\nu7Wu4Ta7oCUCACc7Ua4R8/Xlbpo7WvzLF0CY+TcV20y3AwCUV2v0AsVFa9kkxUWbTgMg8nBW\nJgAEh8/S1KX66JS2ZalDvOk0ACKS38WudlNdzYpdvTPtACBifedlrduvTTPUK9l0FACRiluK\nAUAQ/LFA/9ih1VN0WYrpKAAimB/DE7WLcw132lH4AESyRbv1w7Wana5b+5mOAiCycUAxALTJ\n5kPKWKbf3qapl5mOAiDiUewAIHB7ijQmV9MG6XtDTUcBgLZPxQbxImy9sYxW/kR1X9z8OwBA\ncB0tVWq2hvbW39NMRwEASX4VO8uy6t5qom7TanuR8ng8tW9S9+vmf4p6t8Fo/h0AIIjKqjQ2\nV90SlTtB0WwzBmAPfh9Q3HDlzFSFaqbJ1eSk2wEIEa+le5aoqEz5mUqKNZ0GAD7n96XYULSl\nAGoZvQ2AQQ+tVv7H2jpT3ZNMRwGAOlxy54l6XZDaByCI1uzTsj1R+453vqBb1JiL9MYRPfuW\n1k/ThV1MJwOA/xZIsQvu7rrAAjS6Cc+v4YkzZ858+umnjT519OhRy7K8Xm8bcwbM5/NJMhgA\nwcJH6XQVXk1ZGrV6nyf9QuvSrlWHy2JH53q8lnLG+a7tafHBOpfP5+MPpjuY/Sgtyzp27Nj+\n/fsbfTYlJSUpKdyr+v6tbLVmNDWQEA2KWvNLbo0+6++KXUZGxvPPP9/UszExMYcOHWohNwC3\ne+L1cxZ9kLBwZPEFHaslbT0SN3l1l4QY37Qvl/342hLT6QAY1q9fvx49ehw8eLDRZ6dOnTpv\n3rwwR/Kj2DW6HhaUE0b8KnYttroW36FGWVnZsWPHGn1q06ZNX//618+cOdNy9NAoLy8vLy/v\n3LmzqQAIlsrKylOnTnXv3t10EATCZ+lLf45+/BbfzMstr9e7Ze9n41Z2+8bVVv9O1k83RB3+\ntjeKYVhnKiws7NixY1xcnOkgaKvi4uKEhISEhARTAZKSkp566qnhw4c3+myPHj0SExPDHKmt\nwxONzsk6QmJiYv/+/Rt9au/evZKio6PDm+gLUVFRZgMgWPgoHa3ojI6XaVjfqOho7T6uyas7\npw6wfnOr570TnuNlKq6ITmFywrGioqL4g+kOZj9Kj8fTo0ePpuqEEba480S9dsjoAwA7iI+W\npLIqbTig4c9FX9Ojas4on8ejsqovngUAW3HAVGxrNs8xFQsg6Dq205e76Zev6ZUPdf9V1g8H\nnYyL7i5p6R59uZs6tjOdDwAa8GPFrqYq1bvwWvPDtreo2ttaBNzJ2v4OAFCXZenL3fTiBxp9\nkZ68zRflUbVPf31dT27VL240HQ4AGuPHil2jNxNr9JGAm1mLjzf/zvQ5AMFS4VXWCr20Tw9f\np/n/Vsofo89N6nrkTHR0lP4xUnd/xXQ+AGiMAy7FAkCYFZdrXJ72FmvjdF3ZU7+8Sa8d8L3z\nSfklvaJv6hfViYuwAOzKj2LHehiASPDhSaVlKz5G27LUO1mSOrVT+oXW4I5nundPYpISgJ3Z\nYioWAGyi4BMNma3zOmrzjP+0OgBwEL+LXe2AQt0fBjsVABiweLdufU5jL9aLk5UcbzoNAPjP\nv2JHhwPgVrO2a9Ji/eB6/XOkYriYAcCZ/J6KrXfoSe0hI+zAA+BQ1T49tEbP/5+WTNTogabT\nAEAbMBULIKKVVGriIr15TK9N19Xnmk4DAG1DsQMQuT46pbQceaRtWerbwXQaAGiztm4kYdcd\nAId6/bAGz9a552hrJq0OgEsEckuxmjJXdx6WDXYAnGXZe7r5OaVdqNVT1IEBWABu4d+KXaMF\njlYHwFlmbdddC/X9oZqTrlgGYAG4iN977KhxAJzLa+lbL+npnZo3VpMvNZ0GAIKN4QkAkaK0\nUvcsUcHHWpuhG/qYTgMAIUCxAxARjpRo1AKdrlB+pi7sYjoNAIRGq3aXeOpozeMAYCtvF2rI\nHLWLodUBcLmWi13DMiduEQvAOV75UMPmanAvrc9Qt0TTaQAglFoodnUPNKkdm6j3YN2nAMBW\nnt6ltBxNv1y549WOvScA3K5Vf8/V9raaO8OK2VgAtmdZemyjntiiOenKGGQ6DQCEBf+ABeBC\nZ6s1fble+VAv3aubzzOdBgDChWIHwG2OlSp9gYrKtDVTF3c1nQYAwogz1wG4yrvHNWSOojwq\nyKLVAYg4FDsA7rH+gIbN1ZU9tWGaUpJMpwGAsKPYAXCJZ97SiPnKGKRFdykx1nQaADChVXvs\nGh5Z1/AR5mQBmFIzAPv4Zs36qh64xnQaADCH4QkAzlbhVeYLeuF9LZ+ktAGm0wCAUS0UO9bh\nANhZcbnG5mlfsTbN0BU9TKcBANNYsQPgVB+eVFq24mO0LUu9k02nAQAbYHgCgCMVfKIhs3Ve\nR22eQasDgP9o+V6xDeckgvh6AAjAot265TmNvVgvTlZyvOk0AGAbLRS7mj12ralrta9hWx6A\nkJq1XZOX6Jc36Z8jFcNVBwCoo+U9dpZl1TS21izF0eoAhE61Tw+u1vx/a/HdGj3QdBoAsJ9W\nDU/Urtu1+BoACJGTZzU+T+8V6bXpuvpc02kAwJb8mIqlugEw5cAppWUrOkoFWerbwXQaALAr\n9qcAsLvXD2vIbH0pWVtm0uoAoDkUOwC2tuw93fyc0i7U6inqwAAsADSLYgfAvmZt110L9f2h\nmpOuWP66AoCWcOcJAHbktfTIGs3epXljNflS02kAwCEodgBsp7RS9yxRwcdam6Eb+phOAwDO\n4Uexqz3uhPFYAKFzpESjFuh0hfIzdWEX02kAwFEC2bTi+VzQ0wCIcP/+VINnq12MCmh1AOA/\nP4qd9bnaR6h3AILo5Q91wzMa0lvrM9Q10XQaAHCgQFbsrDpUZwGPkgcgYE/v0qgcfeNq5Y5X\nO3b/AkBAgvzXZ023YxMegNazLD22UU9s0ex0ZQwynQYAnCyQYldvZa7exdm2JgIQSc5UacoS\nbTqol+/VTeeZTgMADhfIVGwtVuYAtMWxUqUvUFGZtmbq4q6m0wCA87Vpj11TT7U5FQD3e/e4\nBs9WlEfbsmh1ABAcbZqKBYDArD+gYXN19bl6dbq6J5lOAwBuwc0XAYTbM29pxHxlDNLCu5TA\nACwABI9/f6c2HHplDBZA69UMwD6+WbO+qgeuMZ0GAFynTcMTdZ+i2wFoXoVXM1/Qive1fJLS\nBphOAwBu5PdVkHoFzrIsjjgB0KLico3N04fF2jRDV/QwnQYAXIrtLQBC7sOTSstWfIy2ZalX\nsuk0AOBeDE8ACK38jzVktvp10paZtDoACC2/i129C69chwXQjEW7des8jb1YK+/ROXGm0wCA\n2/lxKbZ2Ox23oADQGrO263uv6I936OHrTEcBgMjg3x67mgJXt9hR6QA0VOnV11ZqyW4tuVvp\nA02nAYCIEcjwBGUOQDNOntX4PL1XpI0zdFVP02kAIJIwFQsgmA6cUlq2oqO0LUt9OphOAwAR\nJghTsR6PhxEKAJK2H9aQ2eqVrC0zaXUAYIB/xc7TmBAlA+AsS/foluc08kKtmqIO8abTAEBE\n8qPYNdPh2HUHRLhZ23X3In1/qGanK5bzMQHAkEBuKVbT8GrLHIt2QCTzWnpkjWbv0vPjdM8l\nptMAQGQL5LiTeveHrfkhi3ZABCqt1KTF2vaJ1mVoWB/TaQAg4jEVCyBAR0o0MkcllSrI0oDO\nptMAAAIYnmj4NZdigQj07081eLYSY1WQSasDALto071imYoFItPLH+qGZzS0t9ZlqGui6TQA\ngM/5Uezq7qKrt6OODXZA5PjXTo3K0bcHa8F4tWM3BwDYSSDDEw2/BhAJvJZ+ul5/3qY5ozX1\nMtNpAAAN+FHs6p1yAiCinKnSlCXadFAv36ubzjOdBgDQGK6jAGjZ0VKlL1BxufIzdVFX02kA\nAE3wb49dvRPsAESCdwo1ZLZiolRAqwMAe/PvlmI1rY47xgKRY91+DZurq8/VhmnqnmQ6DQCg\nWdzTEUCT5r6p1GxNu1wL71ICGzcAwPb8+KuasQkgcliWHtuoxzfrLyP0jatNpwEAtA7/BgdQ\nX4VXM1/Qyvf1wiSlDjCdBgDQan4fd9IU1vMAdzhRrrG52n9Sm2bo8h6m0wAA/MGKHYAv7CtW\nWo4SYrQtS72STacBAPjJ7+NO6ql9KjTxAIRP/scaMkf9O2nzTFodADhSW6diayodx50ATrfw\nXd06T5Mv1YuTdU6c6TQAgIBwKRaIdJal3+fr0Q3605166FrTaQAAbUCxAyJapVdfW6klu7V0\nokZdaDoNAKBtgjYVC8BxTp7VuDy9X6SNM3RVT9NpAABtFpwVO4YnAMc5cEpp2YqO0rYs9elg\nOg0AIBjaOhVbdzYWgFNsP6zBs9UrWVtm0uoAwD24VywQcZbs0c3PatSFWjVFHeJNpwEABI9/\nxc7j8dTbadfwEQB2Nmu7Ji7SD67X7HTF8i87AHCX4AxPeDweLsgCNue19PAazdml+eM06RLT\naQAAIeD38ES9AmdZFit2gP2VVmrSYm37ROsyNKyP6TQAgNDgHDvA/Q6XaFSOSipVkKUBnU2n\nAQCEDFtsAJf796caPFuJsSrIpNUBgMv5XewaDk8ELwyAIHtpn4bN1a39tGGauiaaTgMACDH/\nzrGr+cJTR72nANjHv3YqfYG+M0TPjFZctOk0AIDQ82+PXU2Bq7tKR6UDbMhr6Tsv66kdmjta\n915mOg0AIFwCGZ6gzAF2dqZKk5doyyG9MlU39jWdBgAQRkzFAq5ytFTpC1Rcrq0zdVFX02kA\nAOHFnScA93inUENmKyZKBZm0OgCIRH4Uu+bvPBGMMAACt26/hs3VNV/ShmnqnmQ6DQDABL+P\nO2l454nghQEQoLlvKjVb0y5X3gQlsMMCACIV3wEAZ7MsPbZRj2/WX0fo61ebTgMAMIpiBzhY\nhVczluvFD7TiHo24wHQaAIBpfhc7j8dT9/Iru+sAU06Ua2yu9p/Uphm6vIfpNAAAG/Cj2FmW\nVVPjGpY5dtoBYbavWGk5SojRtiz1SjadBgBgD/4NT1iW1XB4glYHhNnWjzVkjr7cTfmZtDoA\nwBdsdOeJ5u8829Q139p1xEafClY2wD7y3tX05brvKv35TkWxFQIAUIddhifqbt2rt42vRjNt\nz68iCDhX7QDsn+7UQ9eaTgMAsJ8gFLtm2lVINdr/WnwKcKhKr7JWaOkeLZ2oUReaTgMAsKXA\ni10Ql8Tq9bCaq6vNNzNaHSLKybMal6f3i7Rxhq7qaToNAMCuAjnupN4jtCggpPafVFqOYqO0\nLUt9OphOAwCwMT+KnX0qXVCW60pKSo4fP97oU0ePHrUsy+v1Bh6xbXw+nySDARAsbf8otx/2\njF0YNSjFyh3v6xAvflOYUvMh8qfSNXw+H5+mO5j9KC3LOnbs2P79+xt9tlu3buecc06YI7Wq\nBtWtdDWvD+6+uoZtrPl+FpRil5GR8fzzzzf1bExMzKFDh1rzPkDorDrQ7qENHccNKH9i2Gex\nft/YGQAQWv369UtJSWmqMEydOnXevHlhjtTyVrbarxvecMJIsQvW7rqqqqrS0tJGn9qwYcOU\nKVPKyspa+VZBV15eXlZW1qVLF1MBECyVlZUnT55MSUkJ4L/9y+ue773i+ekN1i9uZLeDeV6v\nt6ioqGvXrtHR0aazoK0+/fTTTp06xcXFmQ6Ctjpx4kRiYmJCQoKpAElJSc8///ytt97a6LPt\n27ePjY0NcyS7HHcSfrGxsZ06dWr0qfbt20uKijK2QlLTmw0GQLAE9lFW+/TwGj3zluaP06RL\nPBJn95hX84/GqKgo/mC6g8fj4aN0B+MfZfv27ZuqE0a0UOzqXngN3bEm9cZgGWtFJCut1MTF\nev2w1mXo+t6m0wAAHKVVK3YN6104tbLzUQfhAodLNDJHZyqVn6kBnU2nAQA4jR+rlw1vC+vx\neILV82oW7WrQzxCZ/u9TDZ6tzgl6/Wu0OgBAIPy+LG19rvaRYNW7hu9c+3ijXzfzMsBxXtqn\nG+bqtv5aM0Ud25lOAwBwpsD3GzbVwwD4a9Z2jczRd4bomdGKY+YSABCoIEzF0u2AgHktfedl\n/XOHnh2jey8znQYA4HCRe9wJYNyZKk1eoi2H9PJU3djXdBoAgPNR7AAzjpZqVI5OnVV+pgZy\nFjUAIBg4nhEw4J1CDZ6tuGgVZNHqAABBQ7EDwm3tfg2bq2u/pPXT1C3RdBoAgItQ7ICwmvOm\n0rI17XLlTVACWyEAAEHFNxYgTCxLj23U45v11xH6+tWm0wAA3IhiB4RDhVfTl2vVB1pxj0Zc\nYDoNAMClKHZAyJ0o15hcfXRKm2dqUIrpNAAA92KPHRBaH52OGTJblV7tuI9WBwAILYodEEIF\nh6NGLu9ySXe9Ok0pSabTAADcjmIHhMpz/6c7c2LHXlC++G4lxppOAwCIAOyxA4KvdgD2yVur\nJ/Y7HeVhsQ4AEA4UOyDIKr3KXKHl72nZRN3e11tcbDoQACBiUOyAYCou17g87S3Wxum6sqcq\nKkwHAgBEEoodEDT7TyotR7FRKshUnw6m0wAAIg/DE0BwbPtEQ+aoTwdtmUmrAwCYQbEDgmDx\nbt3ynNIHatVkJcebTgMAiFQUO6CtZm3XpMX6wfV6epRi+CMFADCHPXZA4Kp9emiNnn1L2eM1\n8Sum0wAAIh7FDghQSaUmLtIbR7QuQ9f3Np0GAACKHRCYwyUamaNKr974ms7raDoNAACS2GMH\nBOD/PtXg2eqSoK0zaXUAABuh2AH+Wf6ehs7Rbf215l51bGc6DQAAdVDsAD/M2q4JC/X9oXpm\ntGL50wMAsBn22AGt4rX07Zf0r516bqymXGo6DQAAjaHYAS07U6V7Fmvrx3plqob3NZ0GAIAm\nUOyAFhwt1agcnTqr/EwN7GI6DQAATWOXENCctws1eLbiolWQRasDANgdxQ5o0tr9umGurvuS\n1k9Tt0TTaQAAaAnFDmjc7F1Ky9a0y5U7QQnsWQAAOAHfr4D6LEuPbdTjm/W3VN1/lek0AAC0\nGsUO+C9nqzXjBa36QCvv0VcvMJ0GAAB/UOyAL5wo15hcHT6tbVn6cjfTaQAA8BPFDviPvcVK\ny1anBBVkKSXJdBoAAPzH8AQgSRsO6NqndUl3vTqNVgcAcCqKHaBn39KIbGUM0uK7lRhrOg0A\nAIHiUiwiWu0A7P9+Vd+8xnQaAADahmKHyFXhVdYKLX9PyyZq5IWm0wAA0GYUO0So4nKNy9Pe\nYm2crit7mk4DAEAwUOwQifafVFqOYqO0LUu9k02nAQAgSBieQMTZ9omGzFGfDtoyk1YHAHAV\nih0iy+LduuU5jR6oVZOVHG86DQAAQUWxQwSZtV2TFusH1+tfoxTD730AgOuwxw4Rodqnh9bo\n2beUPV4Tv2I6DQAAoUGxg/uVVGriIr15TJtm6JpzTacBACBkKHZwucMlGpmjKq+2ZalvB9Np\nAAAIJfYZwc3eOKKr/6UuCdoyk1YHAHA/ih1ca/l7uulZjbhAa+5Vx3am0wAAEHoUO7jTrO2a\nsFDfH6q5oxXLb3MAQGRgjx3cxmvpWy/p6Z2aN1aTLzWdBgCAMKLYwVVKKzV5ibZ+rFemanhf\n02kAAAgvih3c40iJ0hfo1FnlZ2pgF9NpAAAIOzYfwSXeLtSQOYqPUUEWrQ4AEKEodnCDVz7U\nsLm67ktan6FuiabTAABgCMUOjjd7l0bmaPrlypugdmwuAABEML4NwsEsS49t1OOb9fdU3XeV\n6TQAAJhGsYNTna3W9OV65UO9MlU3n2c6DQAANkCxgyMVlWlMro6UaGumLu5qOg0AAPbAHjs4\nz7vHdc3TqvapIItWBwDAFyh2cJj1BzRsrq7sqQ3TlJJkOg0AAHZCsYOTPPuWRsxXxiAtukuJ\nsabTAABgM+yxgzPUDsD+71f1zWtMpwEAwJYodnCACq8yX9AL72v5JKUNMJ0GAAC7otjB7orL\nNTZP+4q1aYau6GE6DQAANkaxg619eFJp2YqP0bYs9U42nQYAAHtjeAL2VfCJhszWeR21eQat\nDgCAllHsYFOLduvW5zT2Yr04WcnxptMAAOAEFDvY0aztumexfnC9/jlSMfwmBQCgddhjB3up\n9unB1Zr/by2ZqNEDTacBAMBRKHawkZJK3b1Ibx3Ta9N19bmm0wAA4DQUO9jFgVMamSOPtC1L\nfTuYTgMAgAOxfQm28PphDZmtc8/R1kxaHQAAAaLYwbxl7+nm55R2oVZPUQcGYAEACBTFDobN\n2q67Fur7QzUnXbH8fgQAoA3YYwdjvJYeWaPZuzRvrCZfajoNAADOR7GDGaWVumeJCj7W2gzd\n0Md0GgAAXIFiBwOOlGjUAp2uUH6mLuxiOg0AAG7BniaE29uFGjxb7WJoyD0BNAAADjVJREFU\ndQAABBnFDmH1yocaNldDemt9hrolmk4DAIC7UOwQPk/vUlqOpl+u3PFqxy4AAACCje+uCAfL\n0mMb9cQWzUlXxiDTaQAAcCmKHULubLWmL9crH+qle3XzeabTAADgXhQ7hNaxUqUvUFGZtmbq\n4q6m0wAA4GrssUMIvXtcQ+YoyqOCLFodAAAhR7FDqKw/oGFzdWVPbZimlCTTaQAAiAAUO4TE\nM29pxHxlDNKiu5QYazoNAACRgT12CLKaAdjHN2vWV/XANabTAAAQSSh2CKYKrzJf+P/t3W2Q\nVfV9B/Df3b0L8iyKYhAQMlGSTmpxdEzAaOLiOJEHHTE+BYX41I5jJ5l26ivfmBdOXziZjplx\n2kQwRQLEB6pUE2tLaUVHpE2NYxujRQFdY7IJQhRZBPZy+mIbZnP37uNd9pz738/n1S7ncPYL\n/733fvf//5+7sfnNeOqGWHJ23mkAYJRR7Bg2+w7F1Y/GW/ti2y1x3hl5pwGA0UexY3i8vT+W\nrI+x5Xj59pg1Oe80ADAquXmCYbD9vViwOuacHC/cotUBQG4UO+r1+OvRujau/lw88/WYPDbv\nNAAwiil21OWBHfH1TXHvV+J7S6PsuwkAcmWPHUPUeSz+/Cfxw9fiieviqnl5pwEAFDuGZv8n\ncc2j8cbe+PdvxAUz8k4DAESEYscQ7P5dLFkfzU2x/fY4a0reaQCA37MrisH5j1/GgtVx5uR4\n8VatDgCKRbFjEJ58Iy5dG0vOiZ+siClugAWAglHsGKgHdsS1j8XdC2PNldHiGwcAisceO/pX\nyeJbz8bqV2Ld8rjx83mnAQB6odjRj4+PxI2bYntb/MvKuHh23mkAgN4VqNiVSqWuD7Is6+No\nle4ndz+n5kUYrPcPxLKN8dHheOm2OOfUvNMAAH0qSrErlUrHq1j3j4/rt+1V/a2aF2FQXmuP\npRti9pR47qaYNj7vNABAfxp4D3y/XZB6PPd2XPyDWDgrtqzU6gCgMRSi2FXVsizLelt47e2v\n9KTn1eOhV2LZhrjzgth4TZxUlFldAKAfSb1o971Lj4HIsvj28/HXL8aaq+Lmc/NOAwAMRkMW\nu5rTdYNdmd23b9/u3btrHtq5c2eWZUePHq0/6tBUKpVcAhw8Giufan6xrenHN3R++awsv/+A\ndHR2dkZEjt9LDJdjx45FRGdnZ9cHNLrOzs6mpkKsWVGPLMsqlUqOz7FZlu3cufO0006reXTu\n3LmnnHLKCEcqxNa0nj2s72Y2kPP77XarVq165JFHejtaLpfffffdfnKn5TcdTd947pT9h5vW\nfXXfZ07uzDsOABTd3Llzp0+f3lthWLly5dq1a0c4UuMVu36n6/q9QpfOzs4DBw7UPLR169YV\nK1Z0dHT0H/3EOHToUEdHx6mnjtz7i/z8t7FsY9OnJsWT1x07fcKIfdn0HTlyZP/+/dOnT887\nCPWqVCp79+6dNm1ac3Nz3lmoV3t7+9SpU8eMGZN3EOr1wQcfjB8/fty4cXkFmDBhwrp16xYt\nWlTz6KRJk8rlkV4abcil2GFRLpenTp1a89DEiRMjIsdZ+q7NgiMWYMuuuPbxWDQ31i2PcWVr\nE8NphIeSE6frB8WmpiajmYZSqWQo05D7UE6cOLG3OpGLQnxbV90G671LRtLDP4vF62Pln8Rj\n18a40dvzASAFDfBKPpC7Irqqobe1G5SuG2DveyG+e0XceUHeaQCAuhWl2HWftBtaJ6v/CqPK\n4Urcujn+8c3YfEMsPjvvNADAcChKsYve21jVexcP4QpU+eBQXP2j2LU/Xrgl5p+RdxoAYJgU\nqNgxMt7aF0s2xLhyvHx7zJycdxoAYPgU4uYJRsxLbbFwTXx6arxwq1YHAKlR7EaRx34eix6J\nqz8XT98Yk7x/EwAkx1LsaPHAjvirf47vXB7f/ELeUQCAE0OxS9+RStzxdGx6PTZdF1fOyzsN\nAHDCKHaJ2/9JXPNovLE3nr8lzv9U3mkAgBNJsUvZ7t/FkvXR3BQv3x6zp+SdBgA4wdw8kawd\nv4wFq2Pm5HjxVq0OAEYFxS5N//CLaF0bS8+JH6+IKWPzTgMAjAjFLkEP7IjrHo+7F8bqK6PF\nCAPAqGGPXVIqWXzz2VjzSqxbHjd+Pu80AMDIUuzS8fGRuOGJePm92LIyvjQ77zQAwIhT7BLx\n/oFYuiEOHIntt8fZp+SdBgDIgx1YKXitPb64Osa3xPbbtDoAGL0Uu4b33Ntx8Q9i4azYsjKm\njc87DQCQH8WusX3/v2LZhviLL8bGa+Ik6+oAMLrpAo2qksU9/xp/83KsuSpuPjfvNABAASh2\nDeng0VixKba9E8/dFF+Zk3caAKAYFLvG86uP48qNse9QvHRbfHZa3mkAgMKwx67B/M9vYsHq\nKDfFdq0OAPhDil0j2bIrvvRwXDAjtq6K0yfknQYAKBjFrmE8/LNYvD5WzY/Hro1xltABgB4U\nhAaQZfHt5+O+F+K7V8SdF+SdBgAoKsWuWA5X4u9+Gtt2j917sHzujPiz8+PsU+PWzfH0m7H5\nhlh8dt75AIACU+wK5H8/iMXr41BnLP1MNmdS53/vHXPe92LmpDh6LLbdEvPPyDsfAFBsil1R\nZFncuCk+Oy1+9LVo6jxy8ODBD5vHf/nv450PY8PXtDoAoH9uniiKn/4qXv11/O3SmDgmIuI/\nf92yYE1ceGZc+0fx1C/yDgcANALFrije2BtnTopZkyMidrzfdPXmk286N564Li6aHW/szTsc\nANAILMUWxUnl6DgaWRalUpw1Jfv+5R+tunByRBw8GicZJQBgAMzYFcWCmbH/k/i3PRERZ0zI\nFs89HBHHstj0eiyclW80AKAxKHZFMXNy/On5cfOT8U9v/f+f/LYjbn4y3t4ff7kg12QAQIOw\nyFcgD3w1xrfElRtj8thxU8eO3fNR/PHpsWVlzJycdzIAoBEodgUypjm+c3l86wuxbdeR9g+P\nXPjpSQtnRXMp71gAQINQ7Apn9pRYPq9y8OAnp502Ke8sAEAjsccOACARih0AQCIUOwCARCh2\nAACJUOwAABKh2AEAJEKxAwBIhGIHAJAIxQ4AIBGKHQBAIhQ7AIBEKHYAAIlQ7AAAEqHYAQAk\nQrEDAEiEYgcAkAjFDgAgEYodAEAiFDsAgEQodgAAiVDsAAASodgBACRCsQMASIRiBwCQCMUO\nACARih0AQCIUOwCARCh2AACJUOwAABKh2AEAJEKxAwBIhGIHAJAIxQ4AIBGKHQBAIhQ7AIBE\nKHYAAIko5x2giMrl8uHDh0ulUt5BAIBCK5eLVaVKWZblnaFwKpXKtm3bKpVKXgGeffbZrVu3\n3n///XkFYLi8+uqrDz744EMPPZR3EOrV3t5+zz333HfffdOnT887C/W644477rrrrvnz5+cd\nhHrdfffdra2tV1xxRV4BmpubL7nkkubm5rwC9FSsmlkQzc3Nl156aY4Bdu3a9cwzz1x22WU5\nZmBYlEqltrY2Q5mAPXv2vPPOOxdddNGcOXPyzkK92trazjvvvEWLFuUdhHp1dHTMmzfPc2x3\n9tgBACRCsQMASIRiBwCQCMUOACARih0AQCIUOwCARCh2AACJaL733nvzzkANTU1Nra2teaeg\nXi0tLQcOHFi2bFneQajXmDFj2trarr/++paWlryzUK/33ntv+fLlU6ZMyTsI9Wpvb29tbZ0x\nY0beQQrEb54AAEiEpVgAgEQodgAAiVDsAAASodgBACRCsQMASIRiBwCQCMUOACARih0AQCLK\neQeghlKp1P1TbyLduLoPpXFsRFUPxuOMZuM6PqYGsdEZypoUu4LybZqAUukPfrNL1ac0hJpD\n1lvbo/i6Pww9JBuaoeyNpVg4ITzRpMrINq6qscuyTEdvUIayD4pd4XjZSJVhTYCHJ1BwlmKL\nyL6BZBhKKI6ueR3rd6RNsSsizztp8BKSGIOYgO5rdkaTJFmKLRz7BpJhKKFoutp5Fw/JxlU1\nfIayO8UOoH+m6xJgx31Kuoavi8dmd5ZiAYDGo8/VZMauWPz4CAB981rZB8WuWHruG/ATSYMy\nlCkxfGnwqEyVoezO/0URuWkrGYYyDV42UuJRmQa/sLE3nq0AABJhKRYAIBGKHQBAIhQ7AIBE\nKHYAAIlQ7AAAEqHYAQAkQrEDAEiEYgcAkAjFDgAgEYodAEAiFDsAgEQodgAAiVDsAAASodgB\nACRCsQMASIRiBwCQCMUOACARih0AQCIUO6CBlUqlUqk02EMn9ORBXQpgeCl2QGM43pZ61iZF\nCqCLYgekoM5ul2VZlmXDFQYgL4odAEAiynkHAOhL96m4qtXY43NsWZZ17Wzrd9at+9W6n1x1\nwZ5n9jyhj6sN9mhvYaq+aN8XBAgzdkAauopO3wuyVUf7vgGi3zP7PmdQn/YdZuCRAMzYAYV2\nvLFVfTBYNefk+r5abxN4PU8YwtF+T+ierY+pO4DuzNgBiRjIpN1A1Fx17e3LDeRoVbCe16+Z\nvO/GaSkWqMmMHTCKNPRclz12QL8UO6C4BnLnRHf93kXRoH2o55TekJekgbRZigWSMlwLsgWU\n/V7eQYDiUuyA4jreY6o+GHK5qZr06uOmh5qThUP+QtFtsrC36/fxj/JryoABshQLpOb4O8DV\n/MNB/U7YemIM9h1MBnVB83ZATWbsgAT1tgOv6tMBbsUbQovq+wo9Lzjwe2yHlgcYJWy/Bejf\nAN8DBSBfZuwAqlXtadPqgEZhjx1Abe5XABqOpViA2o4XO8+TQKNQ7AAAEmGPHQBAIhQ7AIBE\nKHYAAIlQ7AAAEqHYAQAkQrEDAEiEYgcAkAjFDgAgEYodAEAiFDsAgEQodgAAiVDsAAASodgB\nACRCsQMASIRiBwCQiP8DpnpjOu6tRwsAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "R display func"
    }
   ],
   "source": [
    "plot(knnFit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a probar el modelo con los datos de prueba para mostrar la matriz de confusion asi, como otra informacion estadistica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction drugA drugB drugC drugX drugY\n",
       "     drugA     4     1     0     1     1\n",
       "     drugB     0     3     0     1     3\n",
       "     drugC     0     0     2     0     0\n",
       "     drugX     0     0     0    11     3\n",
       "     drugY     1     0     2     0    15\n",
       "\n",
       "Overall Statistics\n",
       "                                          \n",
       "               Accuracy : 0.7292          \n",
       "                 95% CI : (0.5815, 0.8472)\n",
       "    No Information Rate : 0.4583          \n",
       "    P-Value [Acc > NIR] : 0.0001307       \n",
       "                                          \n",
       "                  Kappa : 0.623           \n",
       " Mcnemar's Test P-Value : NA              \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: drugA Class: drugB Class: drugC Class: drugX\n",
       "Sensitivity               0.80000      0.75000      0.50000       0.8462\n",
       "Specificity               0.93023      0.90909      1.00000       0.9143\n",
       "Pos Pred Value            0.57143      0.42857      1.00000       0.7857\n",
       "Neg Pred Value            0.97561      0.97561      0.95652       0.9412\n",
       "Prevalence                0.10417      0.08333      0.08333       0.2708\n",
       "Detection Rate            0.08333      0.06250      0.04167       0.2292\n",
       "Detection Prevalence      0.14583      0.14583      0.04167       0.2917\n",
       "Balanced Accuracy         0.86512      0.82955      0.75000       0.8802\n",
       "                     Class: drugY\n",
       "Sensitivity                0.6818\n",
       "Specificity                0.8846\n",
       "Pos Pred Value             0.8333\n",
       "Neg Pred Value             0.7667\n",
       "Prevalence                 0.4583\n",
       "Detection Rate             0.3125\n",
       "Detection Prevalence       0.3750\n",
       "Balanced Accuracy          0.7832"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "knnPredict <- predict(knnFit, newdata = nc_testing)\n",
    "confusionMatrix(knnPredict, nc_testing$Drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.729166666666667"
      ],
      "text/latex": [
       "0.729166666666667"
      ],
      "text/markdown": [
       "0.729166666666667"
      ],
      "text/plain": [
       "[1] 0.7291667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nc_knn_accuracy <- mean(knnPredict == nc_testing$Drug)\n",
    "nc_knn_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando Random Forest a \"numeric cleaned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "150 samples\n",
       "  6 predictor\n",
       "  5 classes: 'drugA', 'drugB', 'drugC', 'drugX', 'drugY' \n",
       "\n",
       "Pre-processing: centered (7), scaled (7) \n",
       "Resampling: Cross-Validated (10 fold, repeated 3 times) \n",
       "Summary of sample sizes: 135, 134, 136, 135, 134, 135, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  Accuracy   Kappa    \n",
       "  2     0.9131349  0.8698320\n",
       "  4     0.9152579  0.8747097\n",
       "  7     0.8998413  0.8517307\n",
       "\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final value used for the model was mtry = 4."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rfFit <- train(Drug ~ ., data = nc_training, method = \"rf\", trControl = ctrl, preProcess = c(\"center\", \"scale\"))\n",
    "rfFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {},
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAIAAAByhViMAAAACXBIWXMAABJ0AAASdAHeZh94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\nCk1FToMAAAAASUVORK5CYII=",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "R display func"
    }
   ],
   "source": [
    "plot(rfFit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction drugA drugB drugC drugX drugY\n",
       "     drugA     5     0     0     0     0\n",
       "     drugB     0     4     0     0     2\n",
       "     drugC     0     0     4     0     0\n",
       "     drugX     0     0     0    12     3\n",
       "     drugY     0     0     0     1    17\n",
       "\n",
       "Overall Statistics\n",
       "                                          \n",
       "               Accuracy : 0.875           \n",
       "                 95% CI : (0.7475, 0.9527)\n",
       "    No Information Rate : 0.4583          \n",
       "    P-Value [Acc > NIR] : 2.061e-09       \n",
       "                                          \n",
       "                  Kappa : 0.8252          \n",
       " Mcnemar's Test P-Value : NA              \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: drugA Class: drugB Class: drugC Class: drugX\n",
       "Sensitivity                1.0000      1.00000      1.00000       0.9231\n",
       "Specificity                1.0000      0.95455      1.00000       0.9143\n",
       "Pos Pred Value             1.0000      0.66667      1.00000       0.8000\n",
       "Neg Pred Value             1.0000      1.00000      1.00000       0.9697\n",
       "Prevalence                 0.1042      0.08333      0.08333       0.2708\n",
       "Detection Rate             0.1042      0.08333      0.08333       0.2500\n",
       "Detection Prevalence       0.1042      0.12500      0.08333       0.3125\n",
       "Balanced Accuracy          1.0000      0.97727      1.00000       0.9187\n",
       "                     Class: drugY\n",
       "Sensitivity                0.7727\n",
       "Specificity                0.9615\n",
       "Pos Pred Value             0.9444\n",
       "Neg Pred Value             0.8333\n",
       "Prevalence                 0.4583\n",
       "Detection Rate             0.3542\n",
       "Detection Prevalence       0.3750\n",
       "Balanced Accuracy          0.8671"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rfPredict <- predict(rfFit, newdata = nc_testing)\n",
    "confusionMatrix(rfPredict, nc_testing$Drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.875"
      ],
      "text/latex": [
       "0.875"
      ],
      "text/markdown": [
       "0.875"
      ],
      "text/plain": [
       "[1] 0.875"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nc_rf_accuracy <- mean(rfPredict == nc_testing$Drug)\n",
    "nc_rf_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando Naive Bayes a \"numeric cleaned\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizando datos de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>Age</th><th scope=col>Sex</th><th scope=col>Blood_Pressure</th><th scope=col>Cholesterol</th><th scope=col>Na</th><th scope=col>K</th><th scope=col>Drug</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>0.1355932 </td><td>F         </td><td>HIGH      </td><td>HIGH      </td><td>0.73850872</td><td>0.1877280 </td><td>drugY     </td></tr>\n",
       "\t<tr><th scope=row>3</th><td>0.5423729 </td><td>M         </td><td>LOW       </td><td>HIGH      </td><td>0.49786934</td><td>0.8184983 </td><td>drugC     </td></tr>\n",
       "\t<tr><th scope=row>4</th><td>0.2203390 </td><td>F         </td><td>NORMAL    </td><td>HIGH      </td><td>0.16043214</td><td>0.8744853 </td><td>drugX     </td></tr>\n",
       "\t<tr><th scope=row>5</th><td>0.7796610 </td><td>F         </td><td>LOW       </td><td>HIGH      </td><td>0.14934817</td><td>0.1833763 </td><td>drugY     </td></tr>\n",
       "\t<tr><th scope=row>6</th><td>0.1186441 </td><td>F         </td><td>NORMAL    </td><td>HIGH      </td><td>0.44642032</td><td>0.9809025 </td><td>drugX     </td></tr>\n",
       "\t<tr><th scope=row>10</th><td>0.4745763 </td><td>M         </td><td>LOW       </td><td>NORMAL    </td><td>0.06550607</td><td>0.1192046 </td><td>drugY     </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|lllllll}\n",
       "  & Age & Sex & Blood\\_Pressure & Cholesterol & Na & K & Drug\\\\\n",
       "\\hline\n",
       "\t1 & 0.1355932  & F          & HIGH       & HIGH       & 0.73850872 & 0.1877280  & drugY     \\\\\n",
       "\t3 & 0.5423729  & M          & LOW        & HIGH       & 0.49786934 & 0.8184983  & drugC     \\\\\n",
       "\t4 & 0.2203390  & F          & NORMAL     & HIGH       & 0.16043214 & 0.8744853  & drugX     \\\\\n",
       "\t5 & 0.7796610  & F          & LOW        & HIGH       & 0.14934817 & 0.1833763  & drugY     \\\\\n",
       "\t6 & 0.1186441  & F          & NORMAL     & HIGH       & 0.44642032 & 0.9809025  & drugX     \\\\\n",
       "\t10 & 0.4745763  & M          & LOW        & NORMAL     & 0.06550607 & 0.1192046  & drugY     \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | Age | Sex | Blood_Pressure | Cholesterol | Na | K | Drug | \n",
       "|---|---|---|---|---|---|\n",
       "| 1 | 0.1355932  | F          | HIGH       | HIGH       | 0.73850872 | 0.1877280  | drugY      | \n",
       "| 3 | 0.5423729  | M          | LOW        | HIGH       | 0.49786934 | 0.8184983  | drugC      | \n",
       "| 4 | 0.2203390  | F          | NORMAL     | HIGH       | 0.16043214 | 0.8744853  | drugX      | \n",
       "| 5 | 0.7796610  | F          | LOW        | HIGH       | 0.14934817 | 0.1833763  | drugY      | \n",
       "| 6 | 0.1186441  | F          | NORMAL     | HIGH       | 0.44642032 | 0.9809025  | drugX      | \n",
       "| 10 | 0.4745763  | M          | LOW        | NORMAL     | 0.06550607 | 0.1192046  | drugY      | \n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "   Age       Sex Blood_Pressure Cholesterol Na         K         Drug \n",
       "1  0.1355932 F   HIGH           HIGH        0.73850872 0.1877280 drugY\n",
       "3  0.5423729 M   LOW            HIGH        0.49786934 0.8184983 drugC\n",
       "4  0.2203390 F   NORMAL         HIGH        0.16043214 0.8744853 drugX\n",
       "5  0.7796610 F   LOW            HIGH        0.14934817 0.1833763 drugY\n",
       "6  0.1186441 F   NORMAL         HIGH        0.44642032 0.9809025 drugX\n",
       "10 0.4745763 M   LOW            NORMAL      0.06550607 0.1192046 drugY"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cnormalizar los datos en el conjuto de entrenamiento\n",
    "preprocessParams <- preProcess(nc_training, method=c(\"range\"))\n",
    "nc_training_norm <- predict(preprocessParams, nc_training)\n",
    "head(nc_training_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "model fit failed for Fold01.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold02.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold03.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold04.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold05.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold06.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold07.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold08.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold09.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold10.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold01.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold02.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold03.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold04.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold05.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold06.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold07.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold08.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold09.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold10.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold01.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold02.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold03.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold04.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold05.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold06.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold07.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold08.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold09.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold10.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "There were missing values in resampled performance measures.Warning message in train.default(x, y, weights = w, ...):\n",
      "missing values found in aggregated results"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Naive Bayes \n",
       "\n",
       "150 samples\n",
       "  6 predictor\n",
       "  5 classes: 'drugA', 'drugB', 'drugC', 'drugX', 'drugY' \n",
       "\n",
       "Pre-processing: re-scaling to [0, 1] (7) \n",
       "Resampling: Cross-Validated (10 fold, repeated 3 times) \n",
       "Summary of sample sizes: 136, 136, 134, 135, 135, 136, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  usekernel  Accuracy   Kappa    \n",
       "  FALSE            NaN        NaN\n",
       "   TRUE      0.6607937  0.4635181\n",
       "\n",
       "Tuning parameter 'fL' was held constant at a value of 0\n",
       "Tuning\n",
       " parameter 'adjust' was held constant at a value of 1\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final values used for the model were fL = 0, usekernel = TRUE and adjust\n",
       " = 1."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbFit <- train(Drug ~ ., data = nc_training, method = \"nb\", trControl = ctrl, preProcess = c(\"range\"))\n",
    "#bayes <- train(Drug ~ ., data = data.train,  method = \"nb\", trControl=trctrl, tuneLength = 10)\n",
    "nbFit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction drugA drugB drugC drugX drugY\n",
       "     drugA     0     0     0     0     0\n",
       "     drugB     0     0     0     0     0\n",
       "     drugC     0     0     0     0     0\n",
       "     drugX     4     4     4    12     4\n",
       "     drugY     1     0     0     1    18\n",
       "\n",
       "Overall Statistics\n",
       "                                          \n",
       "               Accuracy : 0.625           \n",
       "                 95% CI : (0.4735, 0.7605)\n",
       "    No Information Rate : 0.4583          \n",
       "    P-Value [Acc > NIR] : 0.01494         \n",
       "                                          \n",
       "                  Kappa : 0.424           \n",
       " Mcnemar's Test P-Value : NA              \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: drugA Class: drugB Class: drugC Class: drugX\n",
       "Sensitivity                0.0000      0.00000      0.00000       0.9231\n",
       "Specificity                1.0000      1.00000      1.00000       0.5429\n",
       "Pos Pred Value                NaN          NaN          NaN       0.4286\n",
       "Neg Pred Value             0.8958      0.91667      0.91667       0.9500\n",
       "Prevalence                 0.1042      0.08333      0.08333       0.2708\n",
       "Detection Rate             0.0000      0.00000      0.00000       0.2500\n",
       "Detection Prevalence       0.0000      0.00000      0.00000       0.5833\n",
       "Balanced Accuracy          0.5000      0.50000      0.50000       0.7330\n",
       "                     Class: drugY\n",
       "Sensitivity                0.8182\n",
       "Specificity                0.9231\n",
       "Pos Pred Value             0.9000\n",
       "Neg Pred Value             0.8571\n",
       "Prevalence                 0.4583\n",
       "Detection Rate             0.3750\n",
       "Detection Prevalence       0.4167\n",
       "Balanced Accuracy          0.8706"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbPredict <- predict(nbFit, newdata = nc_testing)\n",
    "confusionMatrix(nbPredict, nc_testing$Drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.625"
      ],
      "text/latex": [
       "0.625"
      ],
      "text/markdown": [
       "0.625"
      ],
      "text/plain": [
       "[1] 0.625"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nc_nb_accuracy <- mean(nbPredict == nc_testing$Drug)\n",
    "nc_nb_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando SVM a \"numeric cleaned\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Support Vector Machines with Linear Kernel \n",
       "\n",
       "150 samples\n",
       "  6 predictor\n",
       "  5 classes: 'drugA', 'drugB', 'drugC', 'drugX', 'drugY' \n",
       "\n",
       "Pre-processing: re-scaling to [0, 1] (7) \n",
       "Resampling: Cross-Validated (10 fold, repeated 3 times) \n",
       "Summary of sample sizes: 135, 135, 135, 135, 136, 134, ... \n",
       "Resampling results:\n",
       "\n",
       "  Accuracy   Kappa   \n",
       "  0.9515943  0.930929\n",
       "\n",
       "Tuning parameter 'C' was held constant at a value of 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "svmlinFit <- train(Drug ~ ., data = nc_training, method = \"svmLinear\", trControl = ctrl, preProcess = c(\"range\"))\n",
    "svmlinFit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction drugA drugB drugC drugX drugY\n",
       "     drugA     5     0     0     0     0\n",
       "     drugB     0     4     0     0     2\n",
       "     drugC     0     0     4     0     0\n",
       "     drugX     0     0     0    12     0\n",
       "     drugY     0     0     0     1    20\n",
       "\n",
       "Overall Statistics\n",
       "                                         \n",
       "               Accuracy : 0.9375         \n",
       "                 95% CI : (0.828, 0.9869)\n",
       "    No Information Rate : 0.4583         \n",
       "    P-Value [Acc > NIR] : 1.646e-12      \n",
       "                                         \n",
       "                  Kappa : 0.9112         \n",
       " Mcnemar's Test P-Value : NA             \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: drugA Class: drugB Class: drugC Class: drugX\n",
       "Sensitivity                1.0000      1.00000      1.00000       0.9231\n",
       "Specificity                1.0000      0.95455      1.00000       1.0000\n",
       "Pos Pred Value             1.0000      0.66667      1.00000       1.0000\n",
       "Neg Pred Value             1.0000      1.00000      1.00000       0.9722\n",
       "Prevalence                 0.1042      0.08333      0.08333       0.2708\n",
       "Detection Rate             0.1042      0.08333      0.08333       0.2500\n",
       "Detection Prevalence       0.1042      0.12500      0.08333       0.2500\n",
       "Balanced Accuracy          1.0000      0.97727      1.00000       0.9615\n",
       "                     Class: drugY\n",
       "Sensitivity                0.9091\n",
       "Specificity                0.9615\n",
       "Pos Pred Value             0.9524\n",
       "Neg Pred Value             0.9259\n",
       "Prevalence                 0.4583\n",
       "Detection Rate             0.4167\n",
       "Detection Prevalence       0.4375\n",
       "Balanced Accuracy          0.9353"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "svmlinPredict <- predict(svmlinFit, newdata = nc_testing)\n",
    "confusionMatrix(svmlinPredict, nc_testing$Drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.9375"
      ],
      "text/latex": [
       "0.9375"
      ],
      "text/markdown": [
       "0.9375"
      ],
      "text/plain": [
       "[1] 0.9375"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nc_svmlin_accuracy <- mean(svmlinPredict == nc_testing$Drug)\n",
    "nc_svmlin_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando red neuronal a \"numeric cleaned\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenamos el multi-layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Multi-Layer Perceptron \n",
       "\n",
       "150 samples\n",
       "  6 predictor\n",
       "  5 classes: 'drugA', 'drugB', 'drugC', 'drugX', 'drugY' \n",
       "\n",
       "Pre-processing: re-scaling to [0, 1] (7) \n",
       "Resampling: Cross-Validated (10 fold, repeated 3 times) \n",
       "Summary of sample sizes: 135, 134, 136, 133, 135, 135, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  size  Accuracy   Kappa    \n",
       "  1     0.7010344  0.5243466\n",
       "  3     0.8740104  0.8147881\n",
       "  5     0.9026448  0.8575253\n",
       "\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final value used for the model was size = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlpFit <- train(Drug ~ ., data = nc_training, method = \"mlp\", trControl = ctrl, preProcess = c(\"range\"))\n",
    "mlpFit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction drugA drugB drugC drugX drugY\n",
       "     drugA     4     1     0     0     0\n",
       "     drugB     1     2     0     0     0\n",
       "     drugC     0     0     4     0     0\n",
       "     drugX     0     0     0    13     0\n",
       "     drugY     0     1     0     0    22\n",
       "\n",
       "Overall Statistics\n",
       "                                         \n",
       "               Accuracy : 0.9375         \n",
       "                 95% CI : (0.828, 0.9869)\n",
       "    No Information Rate : 0.4583         \n",
       "    P-Value [Acc > NIR] : 1.646e-12      \n",
       "                                         \n",
       "                  Kappa : 0.9086         \n",
       " Mcnemar's Test P-Value : NA             \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: drugA Class: drugB Class: drugC Class: drugX\n",
       "Sensitivity               0.80000      0.50000      1.00000       1.0000\n",
       "Specificity               0.97674      0.97727      1.00000       1.0000\n",
       "Pos Pred Value            0.80000      0.66667      1.00000       1.0000\n",
       "Neg Pred Value            0.97674      0.95556      1.00000       1.0000\n",
       "Prevalence                0.10417      0.08333      0.08333       0.2708\n",
       "Detection Rate            0.08333      0.04167      0.08333       0.2708\n",
       "Detection Prevalence      0.10417      0.06250      0.08333       0.2708\n",
       "Balanced Accuracy         0.88837      0.73864      1.00000       1.0000\n",
       "                     Class: drugY\n",
       "Sensitivity                1.0000\n",
       "Specificity                0.9615\n",
       "Pos Pred Value             0.9565\n",
       "Neg Pred Value             1.0000\n",
       "Prevalence                 0.4583\n",
       "Detection Rate             0.4583\n",
       "Detection Prevalence       0.4792\n",
       "Balanced Accuracy          0.9808"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlpPredict <- predict(mlpFit, newdata = nc_testing)\n",
    "confusionMatrix(mlpPredict, nc_testing$Drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.9375"
      ],
      "text/latex": [
       "0.9375"
      ],
      "text/markdown": [
       "0.9375"
      ],
      "text/plain": [
       "[1] 0.9375"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nc_mlp_accuracy <- mean(mlpPredict == nc_testing$Drug)\n",
    "nc_mlp_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando regresion logistica a \"numeric cleaned\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicamos multinomial linear regression para poder tener mas de una clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 26.682838\n",
      "iter  20 value 0.270314\n",
      "iter  30 value 0.000672\n",
      "final  value 0.000086 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 65.070599\n",
      "iter  20 value 60.915912\n",
      "final  value 60.914446 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 26.767014\n",
      "iter  20 value 2.588842\n",
      "iter  30 value 2.423095\n",
      "iter  40 value 2.319908\n",
      "iter  50 value 2.271329\n",
      "iter  60 value 2.206909\n",
      "iter  70 value 2.173249\n",
      "iter  80 value 2.158434\n",
      "iter  90 value 2.149709\n",
      "iter 100 value 2.140991\n",
      "final  value 2.140991 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 23.447263\n",
      "iter  20 value 0.288009\n",
      "iter  30 value 0.000797\n",
      "final  value 0.000056 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 64.795030\n",
      "iter  20 value 59.875202\n",
      "final  value 59.873926 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 23.565118\n",
      "iter  20 value 2.178802\n",
      "iter  30 value 1.909062\n",
      "iter  40 value 1.843977\n",
      "iter  50 value 1.763233\n",
      "iter  60 value 1.722806\n",
      "iter  70 value 1.704733\n",
      "iter  80 value 1.693831\n",
      "iter  90 value 1.687808\n",
      "iter 100 value 1.676844\n",
      "final  value 1.676844 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 23.604586\n",
      "iter  20 value 0.473467\n",
      "iter  30 value 0.000939\n",
      "final  value 0.000077 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 66.396050\n",
      "iter  20 value 60.870940\n",
      "final  value 60.867612 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 23.731422\n",
      "iter  20 value 2.610511\n",
      "iter  30 value 2.359181\n",
      "iter  40 value 2.314363\n",
      "iter  50 value 2.239849\n",
      "iter  60 value 2.181202\n",
      "iter  70 value 2.153024\n",
      "iter  80 value 2.136922\n",
      "iter  90 value 2.128873\n",
      "iter 100 value 2.119083\n",
      "final  value 2.119083 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 26.004867\n",
      "iter  20 value 0.423099\n",
      "iter  30 value 0.000627\n",
      "final  value 0.000089 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 62.920566\n",
      "iter  20 value 58.764309\n",
      "final  value 58.760757 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 26.096018\n",
      "iter  20 value 2.481256\n",
      "iter  30 value 2.310970\n",
      "iter  40 value 2.246612\n",
      "iter  50 value 2.183930\n",
      "iter  60 value 2.131558\n",
      "iter  70 value 2.099559\n",
      "iter  80 value 2.074102\n",
      "iter  90 value 2.063686\n",
      "iter 100 value 2.055926\n",
      "final  value 2.055926 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 27.457660\n",
      "iter  20 value 0.708814\n",
      "iter  30 value 0.001709\n",
      "final  value 0.000074 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 64.291445\n",
      "iter  20 value 59.987894\n",
      "final  value 59.985490 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 27.581325\n",
      "iter  20 value 2.713614\n",
      "iter  30 value 2.467784\n",
      "iter  40 value 2.311774\n",
      "iter  50 value 2.269865\n",
      "iter  60 value 2.214388\n",
      "iter  70 value 2.184238\n",
      "iter  80 value 2.166665\n",
      "iter  90 value 2.155229\n",
      "iter 100 value 2.133811\n",
      "final  value 2.133811 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 22.126188\n",
      "iter  20 value 0.291115\n",
      "iter  30 value 0.000466\n",
      "final  value 0.000061 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 63.607761\n",
      "iter  20 value 60.752288\n",
      "final  value 60.751663 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 22.252890\n",
      "iter  20 value 2.559248\n",
      "iter  30 value 2.377676\n",
      "iter  40 value 2.316898\n",
      "iter  50 value 2.262171\n",
      "iter  60 value 2.209577\n",
      "iter  70 value 2.183846\n",
      "iter  80 value 2.170283\n",
      "iter  90 value 2.160343\n",
      "iter 100 value 2.148958\n",
      "final  value 2.148958 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 24.983869\n",
      "iter  20 value 0.325319\n",
      "iter  30 value 0.000617\n",
      "final  value 0.000080 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 64.121294\n",
      "iter  20 value 60.790241\n",
      "final  value 60.789862 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 25.077433\n",
      "iter  20 value 2.635855\n",
      "iter  30 value 2.420039\n",
      "iter  40 value 2.340796\n",
      "iter  50 value 2.291976\n",
      "iter  60 value 2.218763\n",
      "iter  70 value 2.185055\n",
      "iter  80 value 2.171722\n",
      "iter  90 value 2.165412\n",
      "iter 100 value 2.159174\n",
      "final  value 2.159174 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 20.507162\n",
      "iter  20 value 0.435616\n",
      "iter  30 value 0.000674\n",
      "final  value 0.000093 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 64.394497\n",
      "iter  20 value 58.055133\n",
      "final  value 58.053014 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 20.619470\n",
      "iter  20 value 2.451161\n",
      "iter  30 value 2.220318\n",
      "iter  40 value 2.115306\n",
      "iter  50 value 2.049638\n",
      "iter  60 value 1.994738\n",
      "iter  70 value 1.932636\n",
      "iter  80 value 1.912629\n",
      "iter  90 value 1.900582\n",
      "iter 100 value 1.891773\n",
      "final  value 1.891773 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 29.961336\n",
      "iter  20 value 0.401645\n",
      "iter  30 value 0.000619\n",
      "final  value 0.000080 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 66.200074\n",
      "iter  20 value 62.772509\n",
      "final  value 62.771352 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 30.083755\n",
      "iter  20 value 2.586040\n",
      "iter  30 value 2.429994\n",
      "iter  40 value 2.380074\n",
      "iter  50 value 2.299979\n",
      "iter  60 value 2.232424\n",
      "iter  70 value 2.203723\n",
      "iter  80 value 2.188709\n",
      "iter  90 value 2.181262\n",
      "iter 100 value 2.173353\n",
      "final  value 2.173353 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 19.039996\n",
      "iter  20 value 0.149558\n",
      "iter  30 value 0.000233\n",
      "final  value 0.000061 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 62.234986\n",
      "iter  20 value 58.669036\n",
      "final  value 58.668657 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 19.160236\n",
      "iter  20 value 1.655020\n",
      "iter  30 value 1.591388\n",
      "iter  40 value 1.494038\n",
      "iter  50 value 1.446775\n",
      "iter  60 value 1.416226\n",
      "iter  70 value 1.402987\n",
      "iter  80 value 1.388511\n",
      "iter  90 value 1.378089\n",
      "iter 100 value 1.369077\n",
      "final  value 1.369077 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 26.665166\n",
      "iter  20 value 0.316140\n",
      "iter  30 value 0.000846\n",
      "final  value 0.000055 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 66.624550\n",
      "iter  20 value 60.973876\n",
      "final  value 60.970572 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 26.750322\n",
      "iter  20 value 2.444508\n",
      "iter  30 value 2.280122\n",
      "iter  40 value 2.213106\n",
      "iter  50 value 2.156171\n",
      "iter  60 value 2.118430\n",
      "iter  70 value 2.093164\n",
      "iter  80 value 2.079368\n",
      "iter  90 value 2.067389\n",
      "iter 100 value 2.060060\n",
      "final  value 2.060060 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 22.570934\n",
      "iter  20 value 0.258543\n",
      "iter  30 value 0.000535\n",
      "final  value 0.000071 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 62.513238\n",
      "iter  20 value 59.382814\n",
      "final  value 59.381842 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 22.661141\n",
      "iter  20 value 2.581722\n",
      "iter  30 value 2.393220\n",
      "iter  40 value 2.322397\n",
      "iter  50 value 2.271015\n",
      "iter  60 value 2.209190\n",
      "iter  70 value 2.177675\n",
      "iter  80 value 2.160473\n",
      "iter  90 value 2.150789\n",
      "iter 100 value 2.138769\n",
      "final  value 2.138769 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 26.533533\n",
      "iter  20 value 0.266981\n",
      "iter  30 value 0.000551\n",
      "final  value 0.000071 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 65.113124\n",
      "iter  20 value 59.254492\n",
      "final  value 59.251117 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 26.615084\n",
      "iter  20 value 2.556576\n",
      "iter  30 value 2.354686\n",
      "iter  40 value 2.276498\n",
      "iter  50 value 2.216292\n",
      "iter  60 value 2.123547\n",
      "iter  70 value 2.072003\n",
      "iter  80 value 2.063995\n",
      "iter  90 value 2.057324\n",
      "iter 100 value 2.049991\n",
      "final  value 2.049991 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 23.076057\n",
      "iter  20 value 0.319428\n",
      "iter  30 value 0.001029\n",
      "final  value 0.000094 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 66.536019\n",
      "iter  20 value 60.796378\n",
      "final  value 60.792487 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 23.207843\n",
      "iter  20 value 2.395138\n",
      "iter  30 value 2.123135\n",
      "iter  40 value 2.044003\n",
      "iter  50 value 1.940601\n",
      "iter  60 value 1.871227\n",
      "iter  70 value 1.845418\n",
      "iter  80 value 1.828434\n",
      "iter  90 value 1.820847\n",
      "iter 100 value 1.810301\n",
      "final  value 1.810301 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 28.002424\n",
      "iter  20 value 0.169667\n",
      "iter  30 value 0.000262\n",
      "final  value 0.000067 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 63.708431\n",
      "iter  20 value 60.589384\n",
      "final  value 60.588976 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 28.087381\n",
      "iter  20 value 2.176122\n",
      "iter  30 value 2.031692\n",
      "iter  40 value 1.949194\n",
      "iter  50 value 1.882062\n",
      "iter  60 value 1.836028\n",
      "iter  70 value 1.802584\n",
      "iter  80 value 1.788490\n",
      "iter  90 value 1.780669\n",
      "iter 100 value 1.773041\n",
      "final  value 1.773041 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 23.491881\n",
      "iter  20 value 0.386363\n",
      "iter  30 value 0.001032\n",
      "final  value 0.000086 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 64.263543\n",
      "iter  20 value 60.103813\n",
      "final  value 60.101485 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 23.602390\n",
      "iter  20 value 2.599018\n",
      "iter  30 value 2.371103\n",
      "iter  40 value 2.304629\n",
      "iter  50 value 2.253057\n",
      "iter  60 value 2.193323\n",
      "iter  70 value 2.152423\n",
      "iter  80 value 2.142081\n",
      "iter  90 value 2.136515\n",
      "iter 100 value 2.129543\n",
      "final  value 2.129543 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 24.014860\n",
      "iter  20 value 0.181647\n",
      "iter  30 value 0.000428\n",
      "final  value 0.000055 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 64.129480\n",
      "iter  20 value 60.894713\n",
      "final  value 60.894450 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 24.105670\n",
      "iter  20 value 2.425066\n",
      "iter  30 value 2.285828\n",
      "iter  40 value 2.205561\n",
      "iter  50 value 2.155370\n",
      "iter  60 value 2.105779\n",
      "iter  70 value 2.060765\n",
      "iter  80 value 2.047194\n",
      "iter  90 value 2.036572\n",
      "iter 100 value 2.022695\n",
      "final  value 2.022695 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 20.010773\n",
      "iter  20 value 0.245735\n",
      "iter  30 value 0.000353\n",
      "final  value 0.000089 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 63.786057\n",
      "iter  20 value 60.987386\n",
      "final  value 60.987135 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 20.138621\n",
      "iter  20 value 2.587880\n",
      "iter  30 value 2.414217\n",
      "iter  40 value 2.341038\n",
      "iter  50 value 2.283102\n",
      "iter  60 value 2.196645\n",
      "iter  70 value 2.179397\n",
      "iter  80 value 2.166703\n",
      "iter  90 value 2.158348\n",
      "iter 100 value 2.149874\n",
      "final  value 2.149874 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 23.222956\n",
      "iter  20 value 0.197037\n",
      "iter  30 value 0.000241\n",
      "final  value 0.000062 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 61.558839\n",
      "iter  20 value 58.934489\n",
      "final  value 58.933576 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 23.308850\n",
      "iter  20 value 2.344170\n",
      "iter  30 value 2.198018\n",
      "iter  40 value 2.118922\n",
      "iter  50 value 2.021795\n",
      "iter  60 value 1.987616\n",
      "iter  70 value 1.946377\n",
      "iter  80 value 1.929748\n",
      "iter  90 value 1.915517\n",
      "iter 100 value 1.905613\n",
      "final  value 1.905613 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 23.509646\n",
      "iter  20 value 0.206347\n",
      "iter  30 value 0.000266\n",
      "final  value 0.000068 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 63.048966\n",
      "iter  20 value 60.378130\n",
      "final  value 60.377170 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 23.596702\n",
      "iter  20 value 2.239216\n",
      "iter  30 value 2.037968\n",
      "iter  40 value 1.978150\n",
      "iter  50 value 1.923606\n",
      "iter  60 value 1.867835\n",
      "iter  70 value 1.840170\n",
      "iter  80 value 1.823711\n",
      "iter  90 value 1.806955\n",
      "iter 100 value 1.794727\n",
      "final  value 1.794727 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 25.453800\n",
      "iter  20 value 0.272957\n",
      "iter  30 value 0.000455\n",
      "final  value 0.000060 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 63.467411\n",
      "iter  20 value 60.428884\n",
      "final  value 60.427783 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 25.536347\n",
      "iter  20 value 2.692810\n",
      "iter  30 value 2.507588\n",
      "iter  40 value 2.395673\n",
      "iter  50 value 2.335566\n",
      "iter  60 value 2.228050\n",
      "iter  70 value 2.202739\n",
      "iter  80 value 2.190338\n",
      "iter  90 value 2.180083\n",
      "iter 100 value 2.169274\n",
      "final  value 2.169274 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 22.774256\n",
      "iter  20 value 0.343371\n",
      "iter  30 value 0.000702\n",
      "final  value 0.000099 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 62.598838\n",
      "iter  20 value 58.543560\n",
      "final  value 58.541778 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 22.863593\n",
      "iter  20 value 2.364605\n",
      "iter  30 value 2.217311\n",
      "iter  40 value 2.145131\n",
      "iter  50 value 2.090869\n",
      "iter  60 value 2.031627\n",
      "iter  70 value 2.000470\n",
      "iter  80 value 1.988408\n",
      "iter  90 value 1.979059\n",
      "iter 100 value 1.969936\n",
      "final  value 1.969936 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 26.847966\n",
      "iter  20 value 0.263281\n",
      "iter  30 value 0.000642\n",
      "final  value 0.000087 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 64.728578\n",
      "iter  20 value 61.081730\n",
      "final  value 61.080820 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 26.937544\n",
      "iter  20 value 2.449323\n",
      "iter  30 value 2.294498\n",
      "iter  40 value 2.216384\n",
      "iter  50 value 2.148243\n",
      "iter  60 value 2.067977\n",
      "iter  70 value 2.035452\n",
      "iter  80 value 2.013782\n",
      "iter  90 value 2.000557\n",
      "iter 100 value 1.986328\n",
      "final  value 1.986328 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 21.981947\n",
      "iter  20 value 0.382449\n",
      "iter  30 value 0.000677\n",
      "final  value 0.000092 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 66.796576\n",
      "iter  20 value 61.149860\n",
      "final  value 61.148232 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 22.103868\n",
      "iter  20 value 2.656958\n",
      "iter  30 value 2.432726\n",
      "iter  40 value 2.378798\n",
      "iter  50 value 2.298538\n",
      "iter  60 value 2.221369\n",
      "iter  70 value 2.180606\n",
      "iter  80 value 2.170018\n",
      "iter  90 value 2.159956\n",
      "iter 100 value 2.150901\n",
      "final  value 2.150901 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 24.324898\n",
      "iter  20 value 0.418083\n",
      "iter  30 value 0.000638\n",
      "final  value 0.000083 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 67.742314\n",
      "iter  20 value 61.415426\n",
      "final  value 61.413205 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 24.452851\n",
      "iter  20 value 2.682192\n",
      "iter  30 value 2.431741\n",
      "iter  40 value 2.372150\n",
      "iter  50 value 2.276941\n",
      "iter  60 value 2.214223\n",
      "iter  70 value 2.191321\n",
      "iter  80 value 2.176127\n",
      "iter  90 value 2.169211\n",
      "iter 100 value 2.159630\n",
      "final  value 2.159630 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 24.478219\n",
      "iter  20 value 0.247564\n",
      "iter  30 value 0.001008\n",
      "final  value 0.000066 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 64.274807\n",
      "iter  20 value 60.368469\n",
      "final  value 60.365595 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 24.568649\n",
      "iter  20 value 2.342809\n",
      "iter  30 value 2.163193\n",
      "iter  40 value 2.065698\n",
      "iter  50 value 2.014172\n",
      "iter  60 value 1.964050\n",
      "iter  70 value 1.947386\n",
      "iter  80 value 1.934548\n",
      "iter  90 value 1.927249\n",
      "iter 100 value 1.917456\n",
      "final  value 1.917456 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 23.318868\n",
      "iter  20 value 0.237511\n",
      "iter  30 value 0.000335\n",
      "final  value 0.000087 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 63.192756\n",
      "iter  20 value 60.732991\n",
      "final  value 60.730853 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 23.408174\n",
      "iter  20 value 2.632369\n",
      "iter  30 value 2.425465\n",
      "iter  40 value 2.349563\n",
      "iter  50 value 2.250463\n",
      "iter  60 value 2.219167\n",
      "iter  70 value 2.193180\n",
      "iter  80 value 2.172280\n",
      "iter  90 value 2.157565\n",
      "iter 100 value 2.149345\n",
      "final  value 2.149345 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 21.581278\n",
      "iter  20 value 0.647580\n",
      "iter  30 value 0.003861\n",
      "final  value 0.000067 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 66.116427\n",
      "iter  20 value 60.200723\n",
      "final  value 60.200222 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 21.708813\n",
      "iter  20 value 2.710922\n",
      "iter  30 value 2.324842\n",
      "iter  40 value 2.255357\n",
      "iter  50 value 2.188051\n",
      "iter  60 value 2.136216\n",
      "iter  70 value 2.107003\n",
      "iter  80 value 2.097961\n",
      "iter  90 value 2.086823\n",
      "iter 100 value 2.079729\n",
      "final  value 2.079729 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 21.895533\n",
      "iter  20 value 0.194035\n",
      "iter  30 value 0.001870\n",
      "final  value 0.000066 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 63.414261\n",
      "iter  20 value 58.699220\n",
      "final  value 58.698403 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 21.981586\n",
      "iter  20 value 2.176657\n",
      "iter  30 value 1.931175\n",
      "iter  40 value 1.776265\n",
      "iter  50 value 1.722526\n",
      "iter  60 value 1.678202\n",
      "iter  70 value 1.652586\n",
      "iter  80 value 1.637473\n",
      "iter  90 value 1.623899\n",
      "iter 100 value 1.610786\n",
      "final  value 1.610786 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 24.998625\n",
      "iter  20 value 0.103551\n",
      "iter  30 value 0.000129\n",
      "iter  30 value 0.000066\n",
      "iter  30 value 0.000066\n",
      "final  value 0.000066 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 62.440901\n",
      "iter  20 value 59.643583\n",
      "final  value 59.642847 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 25.087053\n",
      "iter  20 value 1.979667\n",
      "iter  30 value 1.810445\n",
      "iter  40 value 1.757489\n",
      "iter  50 value 1.722099\n",
      "iter  60 value 1.662913\n",
      "iter  70 value 1.643037\n",
      "iter  80 value 1.634986\n",
      "iter  90 value 1.629441\n",
      "iter 100 value 1.622347\n",
      "final  value 1.622347 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 241.415687 \n",
      "iter  10 value 27.009045\n",
      "iter  20 value 2.710385\n",
      "iter  30 value 2.458913\n",
      "iter  40 value 2.405424\n",
      "iter  50 value 2.325067\n",
      "iter  60 value 2.255333\n",
      "iter  70 value 2.225039\n",
      "iter  80 value 2.205594\n",
      "iter  90 value 2.197669\n",
      "iter 100 value 2.184603\n",
      "final  value 2.184603 \n",
      "stopped after 100 iterations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Penalized Multinomial Regression \n",
       "\n",
       "150 samples\n",
       "  6 predictor\n",
       "  5 classes: 'drugA', 'drugB', 'drugC', 'drugX', 'drugY' \n",
       "\n",
       "Pre-processing: re-scaling to [0, 1] (7) \n",
       "Resampling: Cross-Validated (10 fold, repeated 3 times) \n",
       "Summary of sample sizes: 135, 135, 135, 134, 135, 136, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  decay  Accuracy   Kappa    \n",
       "  0e+00  0.9416865  0.9157149\n",
       "  1e-04  0.9504563  0.9288258\n",
       "  1e-01  0.9206151  0.8825428\n",
       "\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final value used for the model was decay = 1e-04."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multinomFit <- train(Drug ~ ., data = nc_training, method = \"multinom\", trControl = ctrl, preProcess = c(\"range\"))\n",
    "multinomFit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probamos el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction drugA drugB drugC drugX drugY\n",
       "     drugA     5     0     0     0     0\n",
       "     drugB     0     4     0     0     1\n",
       "     drugC     0     0     4     0     0\n",
       "     drugX     0     0     0    13     0\n",
       "     drugY     0     0     0     0    21\n",
       "\n",
       "Overall Statistics\n",
       "                                          \n",
       "               Accuracy : 0.9792          \n",
       "                 95% CI : (0.8893, 0.9995)\n",
       "    No Information Rate : 0.4583          \n",
       "    P-Value [Acc > NIR] : 3.148e-15       \n",
       "                                          \n",
       "                  Kappa : 0.9702          \n",
       " Mcnemar's Test P-Value : NA              \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: drugA Class: drugB Class: drugC Class: drugX\n",
       "Sensitivity                1.0000      1.00000      1.00000       1.0000\n",
       "Specificity                1.0000      0.97727      1.00000       1.0000\n",
       "Pos Pred Value             1.0000      0.80000      1.00000       1.0000\n",
       "Neg Pred Value             1.0000      1.00000      1.00000       1.0000\n",
       "Prevalence                 0.1042      0.08333      0.08333       0.2708\n",
       "Detection Rate             0.1042      0.08333      0.08333       0.2708\n",
       "Detection Prevalence       0.1042      0.10417      0.08333       0.2708\n",
       "Balanced Accuracy          1.0000      0.98864      1.00000       1.0000\n",
       "                     Class: drugY\n",
       "Sensitivity                0.9545\n",
       "Specificity                1.0000\n",
       "Pos Pred Value             1.0000\n",
       "Neg Pred Value             0.9630\n",
       "Prevalence                 0.4583\n",
       "Detection Rate             0.4375\n",
       "Detection Prevalence       0.4375\n",
       "Balanced Accuracy          0.9773"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multinomPredict <- predict(multinomFit, newdata = nc_testing)\n",
    "confusionMatrix(multinomPredict, nc_testing$Drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.979166666666667"
      ],
      "text/latex": [
       "0.979166666666667"
      ],
      "text/markdown": [
       "0.979166666666667"
      ],
      "text/plain": [
       "[1] 0.9791667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nc_multinom_accuracy <- mean(multinomPredict == nc_testing$Drug)\n",
    "nc_multinom_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen de \"accuracy\" en \"numeric cleaned\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAABCFBMVEUAAAABAQECAgIFBQUI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\n5b4GtgAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "R display func"
    }
   ],
   "source": [
    "foo <- matrix(c(0.729, 0.875, 0.625, 0.937, 0.937, 0.979), nrow=1, ncol=6)\n",
    "colnames(foo) <- c(\"kNN\", \"Rand. Frst\", \"N. Bayes\", \"SVM\", \"M. Percp.\", \"Logistic\")\n",
    "barplot(foo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando kNN a \"smoothed\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN necesita que las datos esten normalizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Created from 150 samples and 6 variables\n",
       "\n",
       "Pre-processing:\n",
       "  - centered (3)\n",
       "  - ignored (3)\n",
       "  - scaled (3)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols <- names(sm_training)\n",
    "sm_trainX <- sm_training[, cols != \"Drug\"] # selecciona tdoso los datos de entrenamiento exceptuando la columna Drug\n",
    "sm_preProcValue <- preProcess(x = sm_trainX, method = c(\"center\", \"scale\"), tuneLength = 20)\n",
    "sm_preProcValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entrenando el sistema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k-Nearest Neighbors \n",
       "\n",
       "150 samples\n",
       "  6 predictor\n",
       "  5 classes: 'drugA', 'drugB', 'drugC', 'drugX', 'drugY' \n",
       "\n",
       "Pre-processing: centered (7), scaled (7) \n",
       "Resampling: Cross-Validated (10 fold, repeated 3 times) \n",
       "Summary of sample sizes: 135, 135, 136, 135, 136, 133, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  k  Accuracy   Kappa    \n",
       "  5  0.7570343  0.6457641\n",
       "  7  0.7797958  0.6717484\n",
       "  9  0.8097759  0.7146852\n",
       "\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final value used for the model was k = 9."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(500)\n",
    "ctrl <- trainControl(method = \"repeatedcv\", repeats = 3) # repeatedcv is repeated cross validation\n",
    "knnFit <- train(Drug ~ ., data = sm_training, method = \"knn\", trControl = ctrl, preProcess = c(\"center\", \"scale\"))\n",
    "knnFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction drugA drugB drugC drugX drugY\n",
       "     drugA     5     1     0     0     1\n",
       "     drugB     0     3     0     1     3\n",
       "     drugC     0     0    14     0     0\n",
       "     drugX     0     0     0    12     2\n",
       "     drugY     0     0     6     0    16\n",
       "\n",
       "Overall Statistics\n",
       "                                          \n",
       "               Accuracy : 0.7812          \n",
       "                 95% CI : (0.6603, 0.8749)\n",
       "    No Information Rate : 0.3438          \n",
       "    P-Value [Acc > NIR] : 9.941e-13       \n",
       "                                          \n",
       "                  Kappa : 0.7098          \n",
       " Mcnemar's Test P-Value : NA              \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: drugA Class: drugB Class: drugC Class: drugX\n",
       "Sensitivity               1.00000      0.75000       0.7000       0.9231\n",
       "Specificity               0.96610      0.93333       1.0000       0.9608\n",
       "Pos Pred Value            0.71429      0.42857       1.0000       0.8571\n",
       "Neg Pred Value            1.00000      0.98246       0.8800       0.9800\n",
       "Prevalence                0.07812      0.06250       0.3125       0.2031\n",
       "Detection Rate            0.07812      0.04688       0.2188       0.1875\n",
       "Detection Prevalence      0.10938      0.10938       0.2188       0.2188\n",
       "Balanced Accuracy         0.98305      0.84167       0.8500       0.9419\n",
       "                     Class: drugY\n",
       "Sensitivity                0.7273\n",
       "Specificity                0.8571\n",
       "Pos Pred Value             0.7273\n",
       "Neg Pred Value             0.8571\n",
       "Prevalence                 0.3438\n",
       "Detection Rate             0.2500\n",
       "Detection Prevalence       0.3438\n",
       "Balanced Accuracy          0.7922"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "knnPredict <- predict(knnFit, newdata = sm_testing)\n",
    "confusionMatrix(knnPredict, sm_testing$Drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.78125"
      ],
      "text/latex": [
       "0.78125"
      ],
      "text/markdown": [
       "0.78125"
      ],
      "text/plain": [
       "[1] 0.78125"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sm_knn_accuracy <- mean(knnPredict == sm_testing$Drug)\n",
    "sm_knn_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando Random Forest a \"smoothed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "150 samples\n",
       "  6 predictor\n",
       "  5 classes: 'drugA', 'drugB', 'drugC', 'drugX', 'drugY' \n",
       "\n",
       "Pre-processing: centered (7), scaled (7) \n",
       "Resampling: Cross-Validated (10 fold, repeated 3 times) \n",
       "Summary of sample sizes: 136, 134, 135, 135, 136, 134, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  Accuracy   Kappa    \n",
       "  2     0.9030907  0.8557119\n",
       "  4     0.9185470  0.8802630\n",
       "  7     0.8978281  0.8487205\n",
       "\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final value used for the model was mtry = 4."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rfFit <- train(Drug ~ ., data = sm_training, method = \"rf\", trControl = ctrl, preProcess = c(\"center\", \"scale\"))\n",
    "rfFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction drugA drugB drugC drugX drugY\n",
       "     drugA     5     0     0     0     0\n",
       "     drugB     0     4     0     0     2\n",
       "     drugC     0     0    20     0     0\n",
       "     drugX     0     0     0    12     3\n",
       "     drugY     0     0     0     1    17\n",
       "\n",
       "Overall Statistics\n",
       "                                         \n",
       "               Accuracy : 0.9062         \n",
       "                 95% CI : (0.807, 0.9648)\n",
       "    No Information Rate : 0.3438         \n",
       "    P-Value [Acc > NIR] : < 2.2e-16      \n",
       "                                         \n",
       "                  Kappa : 0.8743         \n",
       " Mcnemar's Test P-Value : NA             \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: drugA Class: drugB Class: drugC Class: drugX\n",
       "Sensitivity               1.00000      1.00000       1.0000       0.9231\n",
       "Specificity               1.00000      0.96667       1.0000       0.9412\n",
       "Pos Pred Value            1.00000      0.66667       1.0000       0.8000\n",
       "Neg Pred Value            1.00000      1.00000       1.0000       0.9796\n",
       "Prevalence                0.07812      0.06250       0.3125       0.2031\n",
       "Detection Rate            0.07812      0.06250       0.3125       0.1875\n",
       "Detection Prevalence      0.07812      0.09375       0.3125       0.2344\n",
       "Balanced Accuracy         1.00000      0.98333       1.0000       0.9321\n",
       "                     Class: drugY\n",
       "Sensitivity                0.7727\n",
       "Specificity                0.9762\n",
       "Pos Pred Value             0.9444\n",
       "Neg Pred Value             0.8913\n",
       "Prevalence                 0.3438\n",
       "Detection Rate             0.2656\n",
       "Detection Prevalence       0.2812\n",
       "Balanced Accuracy          0.8745"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rfPredict <- predict(rfFit, newdata = sm_testing)\n",
    "confusionMatrix(rfPredict, sm_testing$Drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.90625"
      ],
      "text/latex": [
       "0.90625"
      ],
      "text/markdown": [
       "0.90625"
      ],
      "text/plain": [
       "[1] 0.90625"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sm_rf_accuracy <- mean(rfPredict == sm_testing$Drug)\n",
    "sm_rf_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando Naive Bayes a \"smoothed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "model fit failed for Fold01.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold02.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold03.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold04.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold05.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold06.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold07.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold08.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold09.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold10.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold01.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold02.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold03.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold04.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold05.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold06.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold07.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold08.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold09.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold10.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold01.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold02.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold03.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold04.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold05.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold06.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold07.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold08.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold09.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message:\n",
      "model fit failed for Fold10.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL\n",
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "There were missing values in resampled performance measures.Warning message in train.default(x, y, weights = w, ...):\n",
      "missing values found in aggregated results"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Naive Bayes \n",
       "\n",
       "150 samples\n",
       "  6 predictor\n",
       "  5 classes: 'drugA', 'drugB', 'drugC', 'drugX', 'drugY' \n",
       "\n",
       "Pre-processing: re-scaling to [0, 1] (7) \n",
       "Resampling: Cross-Validated (10 fold, repeated 3 times) \n",
       "Summary of sample sizes: 135, 135, 136, 134, 135, 134, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  usekernel  Accuracy   Kappa    \n",
       "  FALSE            NaN        NaN\n",
       "   TRUE      0.6599813  0.4650917\n",
       "\n",
       "Tuning parameter 'fL' was held constant at a value of 0\n",
       "Tuning\n",
       " parameter 'adjust' was held constant at a value of 1\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final values used for the model were fL = 0, usekernel = TRUE and adjust\n",
       " = 1."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbFit <- train(Drug ~ ., data = sm_training, method = \"nb\", trControl = ctrl, preProcess = c(\"range\"))\n",
    "#bayes <- train(Drug ~ ., data = data.train,  method = \"nb\", trControl=trctrl, tuneLength = 10)\n",
    "nbFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction drugA drugB drugC drugX drugY\n",
       "     drugA     0     0     0     0     0\n",
       "     drugB     0     0     0     0     0\n",
       "     drugC     0     0     0     0     0\n",
       "     drugX     4     4    19    12     4\n",
       "     drugY     1     0     1     1    18\n",
       "\n",
       "Overall Statistics\n",
       "                                          \n",
       "               Accuracy : 0.4688          \n",
       "                 95% CI : (0.3428, 0.5977)\n",
       "    No Information Rate : 0.3438          \n",
       "    P-Value [Acc > NIR] : 0.02598         \n",
       "                                          \n",
       "                  Kappa : 0.2924          \n",
       " Mcnemar's Test P-Value : NA              \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: drugA Class: drugB Class: drugC Class: drugX\n",
       "Sensitivity               0.00000       0.0000       0.0000       0.9231\n",
       "Specificity               1.00000       1.0000       1.0000       0.3922\n",
       "Pos Pred Value                NaN          NaN          NaN       0.2791\n",
       "Neg Pred Value            0.92188       0.9375       0.6875       0.9524\n",
       "Prevalence                0.07812       0.0625       0.3125       0.2031\n",
       "Detection Rate            0.00000       0.0000       0.0000       0.1875\n",
       "Detection Prevalence      0.00000       0.0000       0.0000       0.6719\n",
       "Balanced Accuracy         0.50000       0.5000       0.5000       0.6576\n",
       "                     Class: drugY\n",
       "Sensitivity                0.8182\n",
       "Specificity                0.9286\n",
       "Pos Pred Value             0.8571\n",
       "Neg Pred Value             0.9070\n",
       "Prevalence                 0.3438\n",
       "Detection Rate             0.2812\n",
       "Detection Prevalence       0.3281\n",
       "Balanced Accuracy          0.8734"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbPredict <- predict(nbFit, newdata = sm_testing)\n",
    "confusionMatrix(nbPredict, sm_testing$Drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.46875"
      ],
      "text/latex": [
       "0.46875"
      ],
      "text/markdown": [
       "0.46875"
      ],
      "text/plain": [
       "[1] 0.46875"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sm_nb_accuracy <- mean(nbPredict == sm_testing$Drug)\n",
    "sm_nb_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando SVM a \"smoothed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Support Vector Machines with Linear Kernel \n",
       "\n",
       "150 samples\n",
       "  6 predictor\n",
       "  5 classes: 'drugA', 'drugB', 'drugC', 'drugX', 'drugY' \n",
       "\n",
       "Pre-processing: re-scaling to [0, 1] (7) \n",
       "Resampling: Cross-Validated (10 fold, repeated 3 times) \n",
       "Summary of sample sizes: 134, 135, 136, 135, 136, 134, ... \n",
       "Resampling results:\n",
       "\n",
       "  Accuracy   Kappa    \n",
       "  0.9481944  0.9246565\n",
       "\n",
       "Tuning parameter 'C' was held constant at a value of 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "svmlinFit <- train(Drug ~ ., data = sm_training, method = \"svmLinear\", trControl = ctrl, preProcess = c(\"range\"))\n",
    "svmlinFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction drugA drugB drugC drugX drugY\n",
       "     drugA     5     0     0     0     0\n",
       "     drugB     0     4     0     0     2\n",
       "     drugC     0     0    20     0     0\n",
       "     drugX     0     0     0    12     0\n",
       "     drugY     0     0     0     1    20\n",
       "\n",
       "Overall Statistics\n",
       "                                          \n",
       "               Accuracy : 0.9531          \n",
       "                 95% CI : (0.8691, 0.9902)\n",
       "    No Information Rate : 0.3438          \n",
       "    P-Value [Acc > NIR] : < 2.2e-16       \n",
       "                                          \n",
       "                  Kappa : 0.9366          \n",
       " Mcnemar's Test P-Value : NA              \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: drugA Class: drugB Class: drugC Class: drugX\n",
       "Sensitivity               1.00000      1.00000       1.0000       0.9231\n",
       "Specificity               1.00000      0.96667       1.0000       1.0000\n",
       "Pos Pred Value            1.00000      0.66667       1.0000       1.0000\n",
       "Neg Pred Value            1.00000      1.00000       1.0000       0.9808\n",
       "Prevalence                0.07812      0.06250       0.3125       0.2031\n",
       "Detection Rate            0.07812      0.06250       0.3125       0.1875\n",
       "Detection Prevalence      0.07812      0.09375       0.3125       0.1875\n",
       "Balanced Accuracy         1.00000      0.98333       1.0000       0.9615\n",
       "                     Class: drugY\n",
       "Sensitivity                0.9091\n",
       "Specificity                0.9762\n",
       "Pos Pred Value             0.9524\n",
       "Neg Pred Value             0.9535\n",
       "Prevalence                 0.3438\n",
       "Detection Rate             0.3125\n",
       "Detection Prevalence       0.3281\n",
       "Balanced Accuracy          0.9426"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "svmlinPredict <- predict(svmlinFit, newdata = sm_testing)\n",
    "confusionMatrix(svmlinPredict, sm_testing$Drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.953125"
      ],
      "text/latex": [
       "0.953125"
      ],
      "text/markdown": [
       "0.953125"
      ],
      "text/plain": [
       "[1] 0.953125"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sm_svmlin_accuracy <- mean(svmlinPredict == sm_testing$Drug)\n",
    "sm_svmlin_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando red neuronal a \"smoothed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Multi-Layer Perceptron \n",
       "\n",
       "150 samples\n",
       "  6 predictor\n",
       "  5 classes: 'drugA', 'drugB', 'drugC', 'drugX', 'drugY' \n",
       "\n",
       "Pre-processing: re-scaling to [0, 1] (7) \n",
       "Resampling: Cross-Validated (10 fold, repeated 3 times) \n",
       "Summary of sample sizes: 135, 136, 135, 135, 136, 135, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  size  Accuracy   Kappa    \n",
       "  1     0.7032738  0.5268810\n",
       "  3     0.8474405  0.7746002\n",
       "  5     0.9025595  0.8574439\n",
       "\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final value used for the model was size = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlpFit <- train(Drug ~ ., data = sm_training, method = \"mlp\", trControl = ctrl, preProcess = c(\"range\"))\n",
    "mlpFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction drugA drugB drugC drugX drugY\n",
       "     drugA     5     3     0     0     0\n",
       "     drugB     0     1     0     0     0\n",
       "     drugC     0     0    20     0     0\n",
       "     drugX     0     0     0    13     0\n",
       "     drugY     0     0     0     0    22\n",
       "\n",
       "Overall Statistics\n",
       "                                          \n",
       "               Accuracy : 0.9531          \n",
       "                 95% CI : (0.8691, 0.9902)\n",
       "    No Information Rate : 0.3438          \n",
       "    P-Value [Acc > NIR] : < 2.2e-16       \n",
       "                                          \n",
       "                  Kappa : 0.936           \n",
       " Mcnemar's Test P-Value : NA              \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: drugA Class: drugB Class: drugC Class: drugX\n",
       "Sensitivity               1.00000      0.25000       1.0000       1.0000\n",
       "Specificity               0.94915      1.00000       1.0000       1.0000\n",
       "Pos Pred Value            0.62500      1.00000       1.0000       1.0000\n",
       "Neg Pred Value            1.00000      0.95238       1.0000       1.0000\n",
       "Prevalence                0.07812      0.06250       0.3125       0.2031\n",
       "Detection Rate            0.07812      0.01562       0.3125       0.2031\n",
       "Detection Prevalence      0.12500      0.01562       0.3125       0.2031\n",
       "Balanced Accuracy         0.97458      0.62500       1.0000       1.0000\n",
       "                     Class: drugY\n",
       "Sensitivity                1.0000\n",
       "Specificity                1.0000\n",
       "Pos Pred Value             1.0000\n",
       "Neg Pred Value             1.0000\n",
       "Prevalence                 0.3438\n",
       "Detection Rate             0.3438\n",
       "Detection Prevalence       0.3438\n",
       "Balanced Accuracy          1.0000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlpPredict <- predict(mlpFit, newdata = sm_testing)\n",
    "confusionMatrix(mlpPredict, sm_testing$Drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.953125"
      ],
      "text/latex": [
       "0.953125"
      ],
      "text/markdown": [
       "0.953125"
      ],
      "text/plain": [
       "[1] 0.953125"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sm_mlp_accuracy <- mean(mlpPredict == sm_testing$Drug)\n",
    "sm_mlp_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando regresion logistica a \"smoothed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 25.658217\n",
      "iter  20 value 0.391599\n",
      "iter  30 value 0.000889\n",
      "final  value 0.000056 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 65.257027\n",
      "iter  20 value 60.375693\n",
      "final  value 60.374298 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 25.757552\n",
      "iter  20 value 2.513924\n",
      "iter  30 value 2.333019\n",
      "iter  40 value 2.238833\n",
      "iter  50 value 2.197563\n",
      "iter  60 value 2.163636\n",
      "iter  70 value 2.132005\n",
      "iter  80 value 2.117876\n",
      "iter  90 value 2.109988\n",
      "iter 100 value 2.098288\n",
      "final  value 2.098288 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 26.309532\n",
      "iter  20 value 0.408213\n",
      "iter  30 value 0.000746\n",
      "final  value 0.000098 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 68.496893\n",
      "iter  20 value 60.559513\n",
      "final  value 60.557303 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 26.402640\n",
      "iter  20 value 2.684771\n",
      "iter  30 value 2.445896\n",
      "iter  40 value 2.372017\n",
      "iter  50 value 2.311024\n",
      "iter  60 value 2.236514\n",
      "iter  70 value 2.215121\n",
      "iter  80 value 2.194212\n",
      "iter  90 value 2.180543\n",
      "iter 100 value 2.170717\n",
      "final  value 2.170717 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 24.830992\n",
      "iter  20 value 0.209944\n",
      "iter  30 value 0.000394\n",
      "final  value 0.000052 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 65.202724\n",
      "iter  20 value 59.971926\n",
      "final  value 59.970307 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 24.915117\n",
      "iter  20 value 2.455118\n",
      "iter  30 value 2.314969\n",
      "iter  40 value 2.230637\n",
      "iter  50 value 2.158559\n",
      "iter  60 value 2.096860\n",
      "iter  70 value 2.061210\n",
      "iter  80 value 2.041158\n",
      "iter  90 value 2.030826\n",
      "iter 100 value 2.021920\n",
      "final  value 2.021920 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 23.695905\n",
      "iter  20 value 0.293689\n",
      "iter  30 value 0.000565\n",
      "final  value 0.000074 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 62.275180\n",
      "iter  20 value 59.247882\n",
      "final  value 59.246900 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 23.785977\n",
      "iter  20 value 2.459024\n",
      "iter  30 value 2.240182\n",
      "iter  40 value 2.156253\n",
      "iter  50 value 2.086210\n",
      "iter  60 value 2.016124\n",
      "iter  70 value 1.988570\n",
      "iter  80 value 1.964304\n",
      "iter  90 value 1.950863\n",
      "iter 100 value 1.940869\n",
      "final  value 1.940869 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 26.098462\n",
      "iter  20 value 0.269363\n",
      "iter  30 value 0.000380\n",
      "final  value 0.000096 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 64.902735\n",
      "iter  20 value 60.332361\n",
      "final  value 60.331667 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 26.180842\n",
      "iter  20 value 2.624039\n",
      "iter  30 value 2.435374\n",
      "iter  40 value 2.340324\n",
      "iter  50 value 2.289269\n",
      "iter  60 value 2.210833\n",
      "iter  70 value 2.173147\n",
      "iter  80 value 2.158910\n",
      "iter  90 value 2.151467\n",
      "iter 100 value 2.141726\n",
      "final  value 2.141726 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 22.044407\n",
      "iter  20 value 0.280281\n",
      "iter  30 value 0.000455\n",
      "final  value 0.000062 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 62.131131\n",
      "iter  20 value 59.914984\n",
      "final  value 59.912867 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 22.138445\n",
      "iter  20 value 2.574528\n",
      "iter  30 value 2.376369\n",
      "iter  40 value 2.295951\n",
      "iter  50 value 2.249360\n",
      "iter  60 value 2.187769\n",
      "iter  70 value 2.154284\n",
      "iter  80 value 2.136995\n",
      "iter  90 value 2.127180\n",
      "iter 100 value 2.112973\n",
      "final  value 2.112973 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 29.318623\n",
      "iter  20 value 0.271031\n",
      "iter  30 value 0.000621\n",
      "final  value 0.000084 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 64.173760\n",
      "iter  20 value 61.290851\n",
      "final  value 61.288324 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 29.405842\n",
      "iter  20 value 2.556761\n",
      "iter  30 value 2.442416\n",
      "iter  40 value 2.355520\n",
      "iter  50 value 2.298292\n",
      "iter  60 value 2.233711\n",
      "iter  70 value 2.190156\n",
      "iter  80 value 2.172118\n",
      "iter  90 value 2.163756\n",
      "iter 100 value 2.152175\n",
      "final  value 2.152175 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 24.690906\n",
      "iter  20 value 0.346143\n",
      "iter  30 value 0.001813\n",
      "final  value 0.000077 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 63.958667\n",
      "iter  20 value 59.087877\n",
      "final  value 59.086268 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 24.820622\n",
      "iter  20 value 1.942412\n",
      "iter  30 value 1.713310\n",
      "iter  40 value 1.630602\n",
      "iter  50 value 1.569271\n",
      "iter  60 value 1.527766\n",
      "iter  70 value 1.505887\n",
      "iter  80 value 1.494977\n",
      "iter  90 value 1.483612\n",
      "iter 100 value 1.475459\n",
      "final  value 1.475459 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 24.447040\n",
      "iter  20 value 0.221118\n",
      "iter  30 value 0.000385\n",
      "final  value 0.000054 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 64.739341\n",
      "iter  20 value 61.306202\n",
      "final  value 61.304240 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 24.561523\n",
      "iter  20 value 2.138717\n",
      "iter  30 value 1.919192\n",
      "iter  40 value 1.831985\n",
      "iter  50 value 1.769868\n",
      "iter  60 value 1.732646\n",
      "iter  70 value 1.708273\n",
      "iter  80 value 1.696536\n",
      "iter  90 value 1.686296\n",
      "iter 100 value 1.677829\n",
      "final  value 1.677829 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 20.039994\n",
      "iter  20 value 0.253983\n",
      "iter  30 value 0.000345\n",
      "final  value 0.000091 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 64.323159\n",
      "iter  20 value 59.740830\n",
      "final  value 59.739891 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 20.158528\n",
      "iter  20 value 2.403911\n",
      "iter  30 value 2.265173\n",
      "iter  40 value 2.201068\n",
      "iter  50 value 2.117795\n",
      "iter  60 value 2.042519\n",
      "iter  70 value 2.008297\n",
      "iter  80 value 1.990372\n",
      "iter  90 value 1.976166\n",
      "iter 100 value 1.962617\n",
      "final  value 1.962617 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 23.553773\n",
      "iter  20 value 0.246178\n",
      "iter  30 value 0.000367\n",
      "final  value 0.000093 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 64.252448\n",
      "iter  20 value 61.287812\n",
      "final  value 61.285964 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 23.644847\n",
      "iter  20 value 2.616910\n",
      "iter  30 value 2.433070\n",
      "iter  40 value 2.356677\n",
      "iter  50 value 2.304375\n",
      "iter  60 value 2.245186\n",
      "iter  70 value 2.202349\n",
      "iter  80 value 2.182509\n",
      "iter  90 value 2.173059\n",
      "iter 100 value 2.162494\n",
      "final  value 2.162494 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 25.607458\n",
      "iter  20 value 0.139231\n",
      "iter  30 value 0.000249\n",
      "final  value 0.000066 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 63.347969\n",
      "iter  20 value 60.338012\n",
      "final  value 60.337618 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 25.699579\n",
      "iter  20 value 2.175784\n",
      "iter  30 value 1.942107\n",
      "iter  40 value 1.846570\n",
      "iter  50 value 1.798590\n",
      "iter  60 value 1.741259\n",
      "iter  70 value 1.721142\n",
      "iter  80 value 1.706371\n",
      "iter  90 value 1.696499\n",
      "iter 100 value 1.685534\n",
      "final  value 1.685534 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 26.908602\n",
      "iter  20 value 0.422623\n",
      "iter  30 value 0.000854\n",
      "final  value 0.000054 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 61.376537\n",
      "iter  20 value 59.449870\n",
      "final  value 59.449077 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 26.992557\n",
      "iter  20 value 2.580299\n",
      "iter  30 value 2.353079\n",
      "iter  40 value 2.273503\n",
      "iter  50 value 2.240266\n",
      "iter  60 value 2.187784\n",
      "iter  70 value 2.155327\n",
      "iter  80 value 2.143367\n",
      "iter  90 value 2.137809\n",
      "iter 100 value 2.124315\n",
      "final  value 2.124315 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 21.931954\n",
      "iter  20 value 0.163287\n",
      "iter  30 value 0.000268\n",
      "final  value 0.000070 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 63.059621\n",
      "iter  20 value 61.023645\n",
      "final  value 61.022509 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 22.024571\n",
      "iter  20 value 2.530706\n",
      "iter  30 value 2.341070\n",
      "iter  40 value 2.252728\n",
      "iter  50 value 2.214695\n",
      "iter  60 value 2.153410\n",
      "iter  70 value 2.119431\n",
      "iter  80 value 2.105713\n",
      "iter  90 value 2.095905\n",
      "iter 100 value 2.088770\n",
      "final  value 2.088770 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 26.436862\n",
      "iter  20 value 0.310830\n",
      "iter  30 value 0.000499\n",
      "final  value 0.000065 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 63.318845\n",
      "iter  20 value 59.247676\n",
      "final  value 59.246443 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 26.519814\n",
      "iter  20 value 2.533757\n",
      "iter  30 value 2.364783\n",
      "iter  40 value 2.285553\n",
      "iter  50 value 2.227891\n",
      "iter  60 value 2.143809\n",
      "iter  70 value 2.115463\n",
      "iter  80 value 2.100144\n",
      "iter  90 value 2.092975\n",
      "iter 100 value 2.086683\n",
      "final  value 2.086683 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 22.403873\n",
      "iter  20 value 0.180096\n",
      "iter  30 value 0.000255\n",
      "final  value 0.000065 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 63.303399\n",
      "iter  20 value 59.263128\n",
      "final  value 59.259627 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 22.498504\n",
      "iter  20 value 2.018534\n",
      "iter  30 value 1.820721\n",
      "iter  40 value 1.720365\n",
      "iter  50 value 1.683939\n",
      "iter  60 value 1.645211\n",
      "iter  70 value 1.627079\n",
      "iter  80 value 1.618000\n",
      "iter  90 value 1.609556\n",
      "iter 100 value 1.605980\n",
      "final  value 1.605980 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 25.518098\n",
      "iter  20 value 0.296227\n",
      "iter  30 value 0.000504\n",
      "final  value 0.000064 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 64.011599\n",
      "iter  20 value 60.080218\n",
      "final  value 60.079276 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 25.612412\n",
      "iter  20 value 2.474750\n",
      "iter  30 value 2.306517\n",
      "iter  40 value 2.223752\n",
      "iter  50 value 2.146448\n",
      "iter  60 value 2.075360\n",
      "iter  70 value 2.032293\n",
      "iter  80 value 2.015548\n",
      "iter  90 value 2.002778\n",
      "iter 100 value 1.996322\n",
      "final  value 1.996322 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 26.309591\n",
      "iter  20 value 0.285022\n",
      "iter  30 value 0.000533\n",
      "final  value 0.000074 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 63.325274\n",
      "iter  20 value 60.453246\n",
      "final  value 60.451998 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 26.399106\n",
      "iter  20 value 2.527345\n",
      "iter  30 value 2.397279\n",
      "iter  40 value 2.323218\n",
      "iter  50 value 2.256787\n",
      "iter  60 value 2.202834\n",
      "iter  70 value 2.148186\n",
      "iter  80 value 2.135507\n",
      "iter  90 value 2.124924\n",
      "iter 100 value 2.113010\n",
      "final  value 2.113010 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 28.485417\n",
      "iter  20 value 0.298016\n",
      "iter  30 value 0.000399\n",
      "final  value 0.000050 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 63.891711\n",
      "iter  20 value 60.900295\n",
      "final  value 60.898969 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 28.570414\n",
      "iter  20 value 2.556429\n",
      "iter  30 value 2.374299\n",
      "iter  40 value 2.307207\n",
      "iter  50 value 2.201479\n",
      "iter  60 value 2.150278\n",
      "iter  70 value 2.120719\n",
      "iter  80 value 2.101183\n",
      "iter  90 value 2.092374\n",
      "iter 100 value 2.080217\n",
      "final  value 2.080217 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 25.979090\n",
      "iter  20 value 0.380290\n",
      "iter  30 value 0.000557\n",
      "final  value 0.000072 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 63.724819\n",
      "iter  20 value 60.172547\n",
      "final  value 60.171388 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 26.103447\n",
      "iter  20 value 2.389309\n",
      "iter  30 value 2.256596\n",
      "iter  40 value 2.188537\n",
      "iter  50 value 2.110159\n",
      "iter  60 value 2.024977\n",
      "iter  70 value 1.990570\n",
      "iter  80 value 1.975362\n",
      "iter  90 value 1.963885\n",
      "iter 100 value 1.951566\n",
      "final  value 1.951566 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 220.492994 \n",
      "iter  10 value 24.233192\n",
      "iter  20 value 0.308211\n",
      "iter  30 value 0.001155\n",
      "final  value 0.000086 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 220.492994 \n",
      "iter  10 value 64.895578\n",
      "iter  20 value 60.820445\n",
      "final  value 60.818867 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 220.492994 \n",
      "iter  10 value 24.331455\n",
      "iter  20 value 2.636315\n",
      "iter  30 value 2.422950\n",
      "iter  40 value 2.337594\n",
      "iter  50 value 2.283336\n",
      "iter  60 value 2.222144\n",
      "iter  70 value 2.168274\n",
      "iter  80 value 2.149285\n",
      "iter  90 value 2.140328\n",
      "iter 100 value 2.131848\n",
      "final  value 2.131848 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 24.491232\n",
      "iter  20 value 0.374011\n",
      "iter  30 value 0.000574\n",
      "final  value 0.000073 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 66.181607\n",
      "iter  20 value 61.287096\n",
      "final  value 61.284258 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 24.626229\n",
      "iter  20 value 2.524611\n",
      "iter  30 value 2.286462\n",
      "iter  40 value 2.227417\n",
      "iter  50 value 2.168625\n",
      "iter  60 value 2.105817\n",
      "iter  70 value 2.081668\n",
      "iter  80 value 2.072102\n",
      "iter  90 value 2.065827\n",
      "iter 100 value 2.060297\n",
      "final  value 2.060297 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 25.358707\n",
      "iter  20 value 0.503772\n",
      "iter  30 value 0.001469\n",
      "final  value 0.000052 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 65.653901\n",
      "iter  20 value 59.894469\n",
      "final  value 59.893060 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 25.485495\n",
      "iter  20 value 2.633756\n",
      "iter  30 value 2.337812\n",
      "iter  40 value 2.255266\n",
      "iter  50 value 2.195486\n",
      "iter  60 value 2.144559\n",
      "iter  70 value 2.103616\n",
      "iter  80 value 2.088599\n",
      "iter  90 value 2.076205\n",
      "iter 100 value 2.061641\n",
      "final  value 2.061641 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 16.429023\n",
      "iter  20 value 0.212996\n",
      "iter  30 value 0.000564\n",
      "final  value 0.000081 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 62.449060\n",
      "iter  20 value 57.754374\n",
      "final  value 57.754112 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 16.533622\n",
      "iter  20 value 2.077399\n",
      "iter  30 value 1.690225\n",
      "iter  40 value 1.609108\n",
      "iter  50 value 1.583761\n",
      "iter  60 value 1.571233\n",
      "iter  70 value 1.561946\n",
      "iter  80 value 1.555659\n",
      "iter  90 value 1.547791\n",
      "iter 100 value 1.543043\n",
      "final  value 1.543043 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 24.967597\n",
      "iter  20 value 0.224160\n",
      "iter  30 value 0.000428\n",
      "final  value 0.000057 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 65.231555\n",
      "iter  20 value 60.994645\n",
      "final  value 60.991727 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 25.057268\n",
      "iter  20 value 2.457993\n",
      "iter  30 value 2.324364\n",
      "iter  40 value 2.265831\n",
      "iter  50 value 2.224074\n",
      "iter  60 value 2.164022\n",
      "iter  70 value 2.131046\n",
      "iter  80 value 2.113532\n",
      "iter  90 value 2.104965\n",
      "iter 100 value 2.100196\n",
      "final  value 2.100196 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 27.433326\n",
      "iter  20 value 0.252702\n",
      "iter  30 value 0.000652\n",
      "final  value 0.000085 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 63.602155\n",
      "iter  20 value 60.823820\n",
      "final  value 60.823274 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 27.519339\n",
      "iter  20 value 2.432138\n",
      "iter  30 value 2.277495\n",
      "iter  40 value 2.179447\n",
      "iter  50 value 2.132637\n",
      "iter  60 value 2.092482\n",
      "iter  70 value 2.063211\n",
      "iter  80 value 2.049923\n",
      "iter  90 value 2.043169\n",
      "iter 100 value 2.037773\n",
      "final  value 2.037773 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 28.139836\n",
      "iter  20 value 0.332465\n",
      "iter  30 value 0.000647\n",
      "final  value 0.000083 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 63.950630\n",
      "iter  20 value 58.923912\n",
      "final  value 58.921683 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 28.232671\n",
      "iter  20 value 2.190688\n",
      "iter  30 value 2.029153\n",
      "iter  40 value 1.943107\n",
      "iter  50 value 1.895101\n",
      "iter  60 value 1.815227\n",
      "iter  70 value 1.784054\n",
      "iter  80 value 1.760260\n",
      "iter  90 value 1.748266\n",
      "iter 100 value 1.735866\n",
      "final  value 1.735866 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 25.229153\n",
      "iter  20 value 0.274629\n",
      "iter  30 value 0.000392\n",
      "final  value 0.000100 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 66.634737\n",
      "iter  20 value 60.631024\n",
      "final  value 60.630264 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 217.274118 \n",
      "iter  10 value 25.331498\n",
      "iter  20 value 2.478079\n",
      "iter  30 value 2.342189\n",
      "iter  40 value 2.279428\n",
      "iter  50 value 2.229402\n",
      "iter  60 value 2.173607\n",
      "iter  70 value 2.120244\n",
      "iter  80 value 2.107622\n",
      "iter  90 value 2.097617\n",
      "iter 100 value 2.090284\n",
      "final  value 2.090284 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 21.763457\n",
      "iter  20 value 0.543575\n",
      "iter  30 value 0.002873\n",
      "final  value 0.000095 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 63.742517\n",
      "iter  20 value 59.990944\n",
      "final  value 59.990012 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 215.664680 \n",
      "iter  10 value 21.927833\n",
      "iter  20 value 2.294677\n",
      "iter  30 value 2.003828\n",
      "iter  40 value 1.958469\n",
      "iter  50 value 1.867863\n",
      "iter  60 value 1.822974\n",
      "iter  70 value 1.804588\n",
      "iter  80 value 1.787480\n",
      "iter  90 value 1.779770\n",
      "iter 100 value 1.769674\n",
      "final  value 1.769674 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 23.714688\n",
      "iter  20 value 0.348526\n",
      "iter  30 value 0.000898\n",
      "final  value 0.000065 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 64.191344\n",
      "iter  20 value 60.743536\n",
      "final  value 60.741860 \n",
      "converged\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 218.883556 \n",
      "iter  10 value 23.810822\n",
      "iter  20 value 2.601354\n",
      "iter  30 value 2.396547\n",
      "iter  40 value 2.337268\n",
      "iter  50 value 2.267712\n",
      "iter  60 value 2.200702\n",
      "iter  70 value 2.179376\n",
      "iter  80 value 2.168403\n",
      "iter  90 value 2.160846\n",
      "iter 100 value 2.152044\n",
      "final  value 2.152044 \n",
      "stopped after 100 iterations\n",
      "# weights:  45 (32 variable)\n",
      "initial  value 241.415687 \n",
      "iter  10 value 26.867292\n",
      "iter  20 value 0.464674\n",
      "iter  30 value 0.001418\n",
      "final  value 0.000088 \n",
      "converged\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Penalized Multinomial Regression \n",
       "\n",
       "150 samples\n",
       "  6 predictor\n",
       "  5 classes: 'drugA', 'drugB', 'drugC', 'drugX', 'drugY' \n",
       "\n",
       "Pre-processing: re-scaling to [0, 1] (7) \n",
       "Resampling: Cross-Validated (10 fold, repeated 3 times) \n",
       "Summary of sample sizes: 135, 134, 136, 134, 135, 135, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  decay  Accuracy   Kappa    \n",
       "  0e+00  0.9504960  0.9287556\n",
       "  1e-04  0.9475397  0.9243440\n",
       "  1e-01  0.9274756  0.8935082\n",
       "\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final value used for the model was decay = 0."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multinomFit <- train(Drug ~ ., data = sm_training, method = \"multinom\", trControl = ctrl, preProcess = c(\"range\"))\n",
    "multinomFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction drugA drugB drugC drugX drugY\n",
       "     drugA     5     0     0     0     0\n",
       "     drugB     0     3     0     0     0\n",
       "     drugC     0     0    20     0     0\n",
       "     drugX     0     0     0    13     1\n",
       "     drugY     0     1     0     0    21\n",
       "\n",
       "Overall Statistics\n",
       "                                          \n",
       "               Accuracy : 0.9688          \n",
       "                 95% CI : (0.8916, 0.9962)\n",
       "    No Information Rate : 0.3438          \n",
       "    P-Value [Acc > NIR] : < 2.2e-16       \n",
       "                                          \n",
       "                  Kappa : 0.9572          \n",
       " Mcnemar's Test P-Value : NA              \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: drugA Class: drugB Class: drugC Class: drugX\n",
       "Sensitivity               1.00000      0.75000       1.0000       1.0000\n",
       "Specificity               1.00000      1.00000       1.0000       0.9804\n",
       "Pos Pred Value            1.00000      1.00000       1.0000       0.9286\n",
       "Neg Pred Value            1.00000      0.98361       1.0000       1.0000\n",
       "Prevalence                0.07812      0.06250       0.3125       0.2031\n",
       "Detection Rate            0.07812      0.04688       0.3125       0.2031\n",
       "Detection Prevalence      0.07812      0.04688       0.3125       0.2188\n",
       "Balanced Accuracy         1.00000      0.87500       1.0000       0.9902\n",
       "                     Class: drugY\n",
       "Sensitivity                0.9545\n",
       "Specificity                0.9762\n",
       "Pos Pred Value             0.9545\n",
       "Neg Pred Value             0.9762\n",
       "Prevalence                 0.3438\n",
       "Detection Rate             0.3281\n",
       "Detection Prevalence       0.3438\n",
       "Balanced Accuracy          0.9654"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multinomPredict <- predict(multinomFit, newdata = sm_testing)\n",
    "confusionMatrix(multinomPredict, sm_testing$Drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.96875"
      ],
      "text/latex": [
       "0.96875"
      ],
      "text/markdown": [
       "0.96875"
      ],
      "text/plain": [
       "[1] 0.96875"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sm_multinom_accuracy <- mean(multinomPredict == sm_testing$Drug)\n",
    "sm_multinom_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen de \"accuracy\" en \"smoothed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAA+VBMVEUAAAABAQECAgIICAgP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",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "R display func"
    }
   ],
   "source": [
    "foo <- matrix(c(0.781, 0.906, 0.469, 0.953, 0.953, 0.968), nrow=1, ncol=6)\n",
    "colnames(foo) <- c(\"kNN\", \"Rand. Frst\", \"N. Bayes\", \"SVM\", \"M. Percp.\", \"Logistic\")\n",
    "barplot(foo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando kNN a \"discretized\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kNN necesita que las datos esten normalizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Created from 162 samples and 6 variables\n",
       "\n",
       "Pre-processing:\n",
       "  - centered (1)\n",
       "  - ignored (5)\n",
       "  - scaled (1)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols <- names(dc_training)\n",
    "dc_trainX <- dc_training[, cols != \"Drug\"] # selecciona tdoso los datos de entrenamiento exceptuando la columna Drug\n",
    "dc_preProcValue <- preProcess(x = dc_trainX, method = c(\"center\", \"scale\"), tuneLength = 20)\n",
    "dc_preProcValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "k-Nearest Neighbors \n",
       "\n",
       "162 samples\n",
       "  6 predictor\n",
       "  5 classes: 'drugA', 'drugB', 'drugC', 'drugX', 'drugY' \n",
       "\n",
       "Pre-processing: centered (15), scaled (15) \n",
       "Resampling: Cross-Validated (10 fold, repeated 3 times) \n",
       "Summary of sample sizes: 146, 146, 145, 146, 148, 144, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  k  Accuracy   Kappa    \n",
       "  5  0.6963640  0.5679659\n",
       "  7  0.6777965  0.5325485\n",
       "  9  0.6941643  0.5548094\n",
       "\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final value used for the model was k = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set.seed(500)\n",
    "ctrl <- trainControl(method = \"repeatedcv\", repeats = 3) # repeatedcv is repeated cross validation\n",
    "knnFit <- train(Drug ~ ., data = dc_training, method = \"knn\", trControl = ctrl, preProcess = c(\"center\", \"scale\"))\n",
    "knnFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction drugA drugB drugC drugX drugY\n",
       "     drugA     0     1     0     1     0\n",
       "     drugB     1     1     0     1     0\n",
       "     drugC     0     0     4     1     1\n",
       "     drugX     2     1     2     4     1\n",
       "     drugY     2     1     2     6    20\n",
       "\n",
       "Overall Statistics\n",
       "                                          \n",
       "               Accuracy : 0.5577          \n",
       "                 95% CI : (0.4133, 0.6953)\n",
       "    No Information Rate : 0.4231          \n",
       "    P-Value [Acc > NIR] : 0.03478         \n",
       "                                          \n",
       "                  Kappa : 0.3436          \n",
       " Mcnemar's Test P-Value : NA              \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: drugA Class: drugB Class: drugC Class: drugX\n",
       "Sensitivity               0.00000      0.25000      0.50000      0.30769\n",
       "Specificity               0.95745      0.95833      0.95455      0.84615\n",
       "Pos Pred Value            0.00000      0.33333      0.66667      0.40000\n",
       "Neg Pred Value            0.90000      0.93878      0.91304      0.78571\n",
       "Prevalence                0.09615      0.07692      0.15385      0.25000\n",
       "Detection Rate            0.00000      0.01923      0.07692      0.07692\n",
       "Detection Prevalence      0.03846      0.05769      0.11538      0.19231\n",
       "Balanced Accuracy         0.47872      0.60417      0.72727      0.57692\n",
       "                     Class: drugY\n",
       "Sensitivity                0.9091\n",
       "Specificity                0.6333\n",
       "Pos Pred Value             0.6452\n",
       "Neg Pred Value             0.9048\n",
       "Prevalence                 0.4231\n",
       "Detection Rate             0.3846\n",
       "Detection Prevalence       0.5962\n",
       "Balanced Accuracy          0.7712"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "knnPredict <- predict(knnFit, newdata = dc_testing)\n",
    "confusionMatrix(knnPredict, dc_testing$Drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.557692307692308"
      ],
      "text/latex": [
       "0.557692307692308"
      ],
      "text/markdown": [
       "0.557692307692308"
      ],
      "text/plain": [
       "[1] 0.5576923"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dc_knn_accuracy <- mean(knnPredict == dc_testing$Drug)\n",
    "dc_knn_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando Random Forest a \"discretized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Random Forest \n",
       "\n",
       "162 samples\n",
       "  6 predictor\n",
       "  5 classes: 'drugA', 'drugB', 'drugC', 'drugX', 'drugY' \n",
       "\n",
       "Pre-processing: centered (15), scaled (15) \n",
       "Resampling: Cross-Validated (10 fold, repeated 3 times) \n",
       "Summary of sample sizes: 145, 146, 146, 146, 145, 148, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  mtry  Accuracy   Kappa    \n",
       "   2    0.7558263  0.6289321\n",
       "   8    0.8465079  0.7832461\n",
       "  15    0.8544374  0.7954793\n",
       "\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final value used for the model was mtry = 15."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rfFit <- train(Drug ~ ., data = dc_training, method = \"rf\", trControl = ctrl, preProcess = c(\"center\", \"scale\"))\n",
    "rfFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction drugA drugB drugC drugX drugY\n",
       "     drugA     2     0     0     0     0\n",
       "     drugB     0     4     0     0     0\n",
       "     drugC     0     0     8     0     1\n",
       "     drugX     1     0     0    12     1\n",
       "     drugY     2     0     0     1    20\n",
       "\n",
       "Overall Statistics\n",
       "                                          \n",
       "               Accuracy : 0.8846          \n",
       "                 95% CI : (0.7656, 0.9565)\n",
       "    No Information Rate : 0.4231          \n",
       "    P-Value [Acc > NIR] : 5.403e-12       \n",
       "                                          \n",
       "                  Kappa : 0.8373          \n",
       " Mcnemar's Test P-Value : NA              \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: drugA Class: drugB Class: drugC Class: drugX\n",
       "Sensitivity               0.40000      1.00000       1.0000       0.9231\n",
       "Specificity               1.00000      1.00000       0.9773       0.9487\n",
       "Pos Pred Value            1.00000      1.00000       0.8889       0.8571\n",
       "Neg Pred Value            0.94000      1.00000       1.0000       0.9737\n",
       "Prevalence                0.09615      0.07692       0.1538       0.2500\n",
       "Detection Rate            0.03846      0.07692       0.1538       0.2308\n",
       "Detection Prevalence      0.03846      0.07692       0.1731       0.2692\n",
       "Balanced Accuracy         0.70000      1.00000       0.9886       0.9359\n",
       "                     Class: drugY\n",
       "Sensitivity                0.9091\n",
       "Specificity                0.9000\n",
       "Pos Pred Value             0.8696\n",
       "Neg Pred Value             0.9310\n",
       "Prevalence                 0.4231\n",
       "Detection Rate             0.3846\n",
       "Detection Prevalence       0.4423\n",
       "Balanced Accuracy          0.9045"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rfPredict <- predict(rfFit, newdata = dc_testing)\n",
    "confusionMatrix(rfPredict, dc_testing$Drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.884615384615385"
      ],
      "text/latex": [
       "0.884615384615385"
      ],
      "text/markdown": [
       "0.884615384615385"
      ],
      "text/plain": [
       "[1] 0.8846154"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dc_rf_accuracy <- mean(rfPredict == dc_testing$Drug)\n",
    "dc_rf_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando Naive Bayes a \"discretized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "model fit failed for Fold01.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., Na....0.666667.0.833333...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold02.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold03.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold04.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold05.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold06.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., Na....0.833333.inf...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold07.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., Na....0.333333.0.5...., K....0.166667.0.333333...., K....0.333333.0.5...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold08.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., K....0.166667.0.333333...., K....0.333333.0.5...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold09.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold10.Rep1: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold01.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., Na....0.666667.0.833333...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold02.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold03.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold04.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold05.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., K....0.166667.0.333333...., K....0.333333.0.5...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold06.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold07.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., Na....0.833333.inf...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold08.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold09.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold10.Rep2: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., Na....0.333333.0.5...., K....0.166667.0.333333...., K....0.333333.0.5...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold01.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold02.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold03.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold04.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold05.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold06.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold07.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., Na....0.833333.inf...., K....0.166667.0.333333...., K....0.333333.0.5...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold08.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., Na....0.666667.0.833333...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold09.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., Na....0.333333.0.5...., K....0.166667.0.333333...., K....0.333333.0.5...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message:\n",
      "model fit failed for Fold10.Rep3: usekernel=FALSE, fL=0, adjust=1 Error in NaiveBayes.default(x, y, usekernel = FALSE, fL = param$fL, ...) : \n",
      "  Zero variances for at least one class in variables: Blood_PressureLOW, Blood_PressureNORMAL, CholesterolNORMAL, Na....0.166667.0.333333...., Na....0.666667.0.833333...., K....0.166667.0.333333...., K....0.666667.0.833333...., K....0.833333.inf....\n",
      "Warning message in nominalTrainWorkflow(x = x, y = y, wts = weights, info = trainInfo, :\n",
      "There were missing values in resampled performance measures.Warning message in train.default(x, y, weights = w, ...):\n",
      "missing values found in aggregated results"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Naive Bayes \n",
       "\n",
       "162 samples\n",
       "  6 predictor\n",
       "  5 classes: 'drugA', 'drugB', 'drugC', 'drugX', 'drugY' \n",
       "\n",
       "Pre-processing: re-scaling to [0, 1] (15) \n",
       "Resampling: Cross-Validated (10 fold, repeated 3 times) \n",
       "Summary of sample sizes: 145, 146, 146, 146, 145, 146, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  usekernel  Accuracy   Kappa    \n",
       "  FALSE            NaN        NaN\n",
       "   TRUE      0.5692239  0.3130084\n",
       "\n",
       "Tuning parameter 'fL' was held constant at a value of 0\n",
       "Tuning\n",
       " parameter 'adjust' was held constant at a value of 1\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final values used for the model were fL = 0, usekernel = TRUE and adjust\n",
       " = 1."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbFit <- train(Drug ~ ., data = dc_training, method = \"nb\", trControl = ctrl, preProcess = c(\"range\"))\n",
    "#bayes <- train(Drug ~ ., data = data.train,  method = \"nb\", trControl=trctrl, tuneLength = 10)\n",
    "nbFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction drugA drugB drugC drugX drugY\n",
       "     drugA     0     0     0     0     0\n",
       "     drugB     0     0     0     0     0\n",
       "     drugC     0     0     5     0     0\n",
       "     drugX     2     1     1     7     0\n",
       "     drugY     3     3     2     6    22\n",
       "\n",
       "Overall Statistics\n",
       "                                          \n",
       "               Accuracy : 0.6538          \n",
       "                 95% CI : (0.5091, 0.7803)\n",
       "    No Information Rate : 0.4231          \n",
       "    P-Value [Acc > NIR] : 0.0006654       \n",
       "                                          \n",
       "                  Kappa : 0.4586          \n",
       " Mcnemar's Test P-Value : NA              \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: drugA Class: drugB Class: drugC Class: drugX\n",
       "Sensitivity               0.00000      0.00000      0.62500       0.5385\n",
       "Specificity               1.00000      1.00000      1.00000       0.8974\n",
       "Pos Pred Value                NaN          NaN      1.00000       0.6364\n",
       "Neg Pred Value            0.90385      0.92308      0.93617       0.8537\n",
       "Prevalence                0.09615      0.07692      0.15385       0.2500\n",
       "Detection Rate            0.00000      0.00000      0.09615       0.1346\n",
       "Detection Prevalence      0.00000      0.00000      0.09615       0.2115\n",
       "Balanced Accuracy         0.50000      0.50000      0.81250       0.7179\n",
       "                     Class: drugY\n",
       "Sensitivity                1.0000\n",
       "Specificity                0.5333\n",
       "Pos Pred Value             0.6111\n",
       "Neg Pred Value             1.0000\n",
       "Prevalence                 0.4231\n",
       "Detection Rate             0.4231\n",
       "Detection Prevalence       0.6923\n",
       "Balanced Accuracy          0.7667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "nbPredict <- predict(nbFit, newdata = dc_testing)\n",
    "confusionMatrix(nbPredict, dc_testing$Drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.653846153846154"
      ],
      "text/latex": [
       "0.653846153846154"
      ],
      "text/markdown": [
       "0.653846153846154"
      ],
      "text/plain": [
       "[1] 0.6538462"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dc_nb_accuracy <- mean(nbPredict == dc_testing$Drug)\n",
    "dc_nb_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando SVM a \"discretized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Support Vector Machines with Linear Kernel \n",
       "\n",
       "162 samples\n",
       "  6 predictor\n",
       "  5 classes: 'drugA', 'drugB', 'drugC', 'drugX', 'drugY' \n",
       "\n",
       "Pre-processing: re-scaling to [0, 1] (15) \n",
       "Resampling: Cross-Validated (10 fold, repeated 3 times) \n",
       "Summary of sample sizes: 147, 145, 147, 146, 145, 145, ... \n",
       "Resampling results:\n",
       "\n",
       "  Accuracy   Kappa    \n",
       "  0.8795044  0.8317248\n",
       "\n",
       "Tuning parameter 'C' was held constant at a value of 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "svmlinFit <- train(Drug ~ ., data = dc_training, method = \"svmLinear\", trControl = ctrl, preProcess = c(\"range\"))\n",
    "svmlinFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction drugA drugB drugC drugX drugY\n",
       "     drugA     4     0     0     0     0\n",
       "     drugB     0     3     0     0     0\n",
       "     drugC     0     0     8     0     2\n",
       "     drugX     0     0     0    12     0\n",
       "     drugY     1     1     0     1    20\n",
       "\n",
       "Overall Statistics\n",
       "                                         \n",
       "               Accuracy : 0.9038         \n",
       "                 95% CI : (0.7897, 0.968)\n",
       "    No Information Rate : 0.4231         \n",
       "    P-Value [Acc > NIR] : 4.966e-13      \n",
       "                                         \n",
       "                  Kappa : 0.8653         \n",
       " Mcnemar's Test P-Value : NA             \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: drugA Class: drugB Class: drugC Class: drugX\n",
       "Sensitivity               0.80000      0.75000       1.0000       0.9231\n",
       "Specificity               1.00000      1.00000       0.9545       1.0000\n",
       "Pos Pred Value            1.00000      1.00000       0.8000       1.0000\n",
       "Neg Pred Value            0.97917      0.97959       1.0000       0.9750\n",
       "Prevalence                0.09615      0.07692       0.1538       0.2500\n",
       "Detection Rate            0.07692      0.05769       0.1538       0.2308\n",
       "Detection Prevalence      0.07692      0.05769       0.1923       0.2308\n",
       "Balanced Accuracy         0.90000      0.87500       0.9773       0.9615\n",
       "                     Class: drugY\n",
       "Sensitivity                0.9091\n",
       "Specificity                0.9000\n",
       "Pos Pred Value             0.8696\n",
       "Neg Pred Value             0.9310\n",
       "Prevalence                 0.4231\n",
       "Detection Rate             0.3846\n",
       "Detection Prevalence       0.4423\n",
       "Balanced Accuracy          0.9045"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "svmlinPredict <- predict(svmlinFit, newdata = dc_testing)\n",
    "confusionMatrix(svmlinPredict, dc_testing$Drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.903846153846154"
      ],
      "text/latex": [
       "0.903846153846154"
      ],
      "text/markdown": [
       "0.903846153846154"
      ],
      "text/plain": [
       "[1] 0.9038462"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dc_svmlin_accuracy <- mean(svmlinPredict == dc_testing$Drug)\n",
    "dc_svmlin_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando red neuronal a \"discretized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Multi-Layer Perceptron \n",
       "\n",
       "162 samples\n",
       "  6 predictor\n",
       "  5 classes: 'drugA', 'drugB', 'drugC', 'drugX', 'drugY' \n",
       "\n",
       "Pre-processing: re-scaling to [0, 1] (15) \n",
       "Resampling: Cross-Validated (10 fold, repeated 3 times) \n",
       "Summary of sample sizes: 146, 147, 146, 145, 147, 146, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  size  Accuracy   Kappa    \n",
       "  1     0.6449899  0.4649465\n",
       "  3     0.8823642  0.8340140\n",
       "  5     0.8828143  0.8347980\n",
       "\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final value used for the model was size = 5."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlpFit <- train(Drug ~ ., data = dc_training, method = \"mlp\", trControl = ctrl, preProcess = c(\"range\"))\n",
    "mlpFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction drugA drugB drugC drugX drugY\n",
       "     drugA     4     1     0     0     0\n",
       "     drugB     1     2     0     0     0\n",
       "     drugC     0     0     8     0     2\n",
       "     drugX     0     0     0    12     0\n",
       "     drugY     0     1     0     1    20\n",
       "\n",
       "Overall Statistics\n",
       "                                          \n",
       "               Accuracy : 0.8846          \n",
       "                 95% CI : (0.7656, 0.9565)\n",
       "    No Information Rate : 0.4231          \n",
       "    P-Value [Acc > NIR] : 5.403e-12       \n",
       "                                          \n",
       "                  Kappa : 0.8398          \n",
       " Mcnemar's Test P-Value : NA              \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: drugA Class: drugB Class: drugC Class: drugX\n",
       "Sensitivity               0.80000      0.50000       1.0000       0.9231\n",
       "Specificity               0.97872      0.97917       0.9545       1.0000\n",
       "Pos Pred Value            0.80000      0.66667       0.8000       1.0000\n",
       "Neg Pred Value            0.97872      0.95918       1.0000       0.9750\n",
       "Prevalence                0.09615      0.07692       0.1538       0.2500\n",
       "Detection Rate            0.07692      0.03846       0.1538       0.2308\n",
       "Detection Prevalence      0.09615      0.05769       0.1923       0.2308\n",
       "Balanced Accuracy         0.88936      0.73958       0.9773       0.9615\n",
       "                     Class: drugY\n",
       "Sensitivity                0.9091\n",
       "Specificity                0.9333\n",
       "Pos Pred Value             0.9091\n",
       "Neg Pred Value             0.9333\n",
       "Prevalence                 0.4231\n",
       "Detection Rate             0.3846\n",
       "Detection Prevalence       0.4231\n",
       "Balanced Accuracy          0.9212"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlpPredict <- predict(mlpFit, newdata = dc_testing)\n",
    "confusionMatrix(mlpPredict, dc_testing$Drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.884615384615385"
      ],
      "text/latex": [
       "0.884615384615385"
      ],
      "text/markdown": [
       "0.884615384615385"
      ],
      "text/plain": [
       "[1] 0.8846154"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dc_mlp_accuracy <- mean(mlpPredict == dc_testing$Drug)\n",
    "dc_mlp_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aplicando regresion logistica a \"discretized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 20.699243\n",
      "iter  20 value 0.204021\n",
      "iter  30 value 0.001053\n",
      "final  value 0.000072 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 61.090372\n",
      "iter  20 value 59.723102\n",
      "iter  30 value 59.703781\n",
      "final  value 59.703779 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 20.804284\n",
      "iter  20 value 1.406155\n",
      "iter  30 value 1.259357\n",
      "iter  40 value 1.213647\n",
      "iter  50 value 1.182295\n",
      "iter  60 value 1.168697\n",
      "iter  70 value 1.156826\n",
      "iter  80 value 1.151238\n",
      "iter  90 value 1.146716\n",
      "iter 100 value 1.144972\n",
      "final  value 1.144972 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 19.757632\n",
      "iter  20 value 0.460600\n",
      "iter  30 value 0.002285\n",
      "final  value 0.000051 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 59.496283\n",
      "iter  20 value 58.353935\n",
      "iter  30 value 58.342581\n",
      "final  value 58.342580 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 19.894613\n",
      "iter  20 value 1.819622\n",
      "iter  30 value 1.573117\n",
      "iter  40 value 1.501522\n",
      "iter  50 value 1.444594\n",
      "iter  60 value 1.399823\n",
      "iter  70 value 1.361537\n",
      "iter  80 value 1.343523\n",
      "iter  90 value 1.331870\n",
      "iter 100 value 1.323598\n",
      "final  value 1.323598 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 27.766578\n",
      "iter  20 value 0.296302\n",
      "iter  30 value 0.000842\n",
      "final  value 0.000085 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 61.226375\n",
      "iter  20 value 59.605736\n",
      "iter  30 value 59.593067\n",
      "final  value 59.593065 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 27.852834\n",
      "iter  20 value 1.515312\n",
      "iter  30 value 1.361304\n",
      "iter  40 value 1.315427\n",
      "iter  50 value 1.283723\n",
      "iter  60 value 1.255400\n",
      "iter  70 value 1.236901\n",
      "iter  80 value 1.225018\n",
      "iter  90 value 1.218812\n",
      "iter 100 value 1.212233\n",
      "final  value 1.212233 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 26.024995\n",
      "iter  20 value 0.521850\n",
      "iter  30 value 0.014015\n",
      "final  value 0.000057 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 60.526436\n",
      "iter  20 value 58.829579\n",
      "iter  30 value 58.805488\n",
      "final  value 58.805486 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 26.128395\n",
      "iter  20 value 1.643882\n",
      "iter  30 value 1.481578\n",
      "iter  40 value 1.424992\n",
      "iter  50 value 1.398438\n",
      "iter  60 value 1.371209\n",
      "iter  70 value 1.352726\n",
      "iter  80 value 1.338713\n",
      "iter  90 value 1.329638\n",
      "iter 100 value 1.325502\n",
      "final  value 1.325502 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 236.587373 \n",
      "iter  10 value 22.154632\n",
      "iter  20 value 0.251042\n",
      "iter  30 value 0.000918\n",
      "final  value 0.000091 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 236.587373 \n",
      "iter  10 value 62.586420\n",
      "iter  20 value 59.853343\n",
      "iter  30 value 59.839487\n",
      "final  value 59.839481 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 236.587373 \n",
      "iter  10 value 22.250880\n",
      "iter  20 value 1.725317\n",
      "iter  30 value 1.549251\n",
      "iter  40 value 1.487042\n",
      "iter  50 value 1.431684\n",
      "iter  60 value 1.394889\n",
      "iter  70 value 1.372795\n",
      "iter  80 value 1.352472\n",
      "iter  90 value 1.345238\n",
      "iter 100 value 1.340826\n",
      "final  value 1.340826 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 24.108715\n",
      "iter  20 value 0.442713\n",
      "iter  30 value 0.001115\n",
      "final  value 0.000075 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 59.963407\n",
      "iter  20 value 58.231209\n",
      "iter  30 value 58.202617\n",
      "iter  30 value 58.202617\n",
      "iter  30 value 58.202617\n",
      "final  value 58.202617 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 24.193742\n",
      "iter  20 value 1.729711\n",
      "iter  30 value 1.569801\n",
      "iter  40 value 1.511471\n",
      "iter  50 value 1.451307\n",
      "iter  60 value 1.400811\n",
      "iter  70 value 1.373721\n",
      "iter  80 value 1.360081\n",
      "iter  90 value 1.354696\n",
      "iter 100 value 1.350379\n",
      "final  value 1.350379 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 20.671555\n",
      "iter  20 value 0.362672\n",
      "iter  30 value 0.001738\n",
      "final  value 0.000087 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 60.776560\n",
      "iter  20 value 59.301741\n",
      "iter  30 value 59.289939\n",
      "final  value 59.289936 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 20.776596\n",
      "iter  20 value 1.528072\n",
      "iter  30 value 1.364523\n",
      "iter  40 value 1.312320\n",
      "iter  50 value 1.288416\n",
      "iter  60 value 1.265485\n",
      "iter  70 value 1.250026\n",
      "iter  80 value 1.240388\n",
      "iter  90 value 1.233924\n",
      "iter 100 value 1.231376\n",
      "final  value 1.231376 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 21.232923\n",
      "iter  20 value 0.163723\n",
      "iter  30 value 0.000211\n",
      "final  value 0.000054 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 60.811192\n",
      "iter  20 value 59.257526\n",
      "iter  30 value 59.226815\n",
      "final  value 59.226811 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 21.332892\n",
      "iter  20 value 1.504596\n",
      "iter  30 value 1.343140\n",
      "iter  40 value 1.284362\n",
      "iter  50 value 1.251375\n",
      "iter  60 value 1.226778\n",
      "iter  70 value 1.210942\n",
      "iter  80 value 1.202710\n",
      "iter  90 value 1.193583\n",
      "iter 100 value 1.187278\n",
      "final  value 1.187278 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 28.770435\n",
      "iter  20 value 0.430538\n",
      "iter  30 value 0.001502\n",
      "final  value 0.000068 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 61.847453\n",
      "iter  20 value 60.503088\n",
      "iter  30 value 60.488562\n",
      "final  value 60.488559 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 28.867504\n",
      "iter  20 value 1.626476\n",
      "iter  30 value 1.478527\n",
      "iter  40 value 1.423828\n",
      "iter  50 value 1.383639\n",
      "iter  60 value 1.346828\n",
      "iter  70 value 1.326887\n",
      "iter  80 value 1.316949\n",
      "iter  90 value 1.312178\n",
      "iter 100 value 1.308911\n",
      "final  value 1.308911 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 19.460925\n",
      "iter  20 value 0.400985\n",
      "iter  30 value 0.000966\n",
      "final  value 0.000069 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 61.002818\n",
      "iter  20 value 58.329774\n",
      "iter  30 value 58.284097\n",
      "final  value 58.284091 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 19.558593\n",
      "iter  20 value 1.630053\n",
      "iter  30 value 1.449973\n",
      "iter  40 value 1.395150\n",
      "iter  50 value 1.349683\n",
      "iter  60 value 1.314473\n",
      "iter  70 value 1.292217\n",
      "iter  80 value 1.279693\n",
      "iter  90 value 1.273914\n",
      "iter 100 value 1.270360\n",
      "final  value 1.270360 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 25.742311\n",
      "iter  20 value 0.426496\n",
      "iter  30 value 0.001494\n",
      "final  value 0.000061 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 62.723106\n",
      "iter  20 value 60.759348\n",
      "iter  30 value 60.738926\n",
      "final  value 60.738924 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 25.833081\n",
      "iter  20 value 1.742931\n",
      "iter  30 value 1.536856\n",
      "iter  40 value 1.478576\n",
      "iter  50 value 1.443465\n",
      "iter  60 value 1.405570\n",
      "iter  70 value 1.381217\n",
      "iter  80 value 1.370261\n",
      "iter  90 value 1.362730\n",
      "iter 100 value 1.359429\n",
      "final  value 1.359429 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 236.587373 \n",
      "iter  10 value 23.745558\n",
      "iter  20 value 0.471190\n",
      "iter  30 value 0.002613\n",
      "final  value 0.000056 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 236.587373 \n",
      "iter  10 value 61.530435\n",
      "iter  20 value 60.044164\n",
      "iter  30 value 60.016998\n",
      "final  value 60.016993 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 236.587373 \n",
      "iter  10 value 23.847293\n",
      "iter  20 value 1.659313\n",
      "iter  30 value 1.454632\n",
      "iter  40 value 1.407689\n",
      "iter  50 value 1.356487\n",
      "iter  60 value 1.321709\n",
      "iter  70 value 1.299776\n",
      "iter  80 value 1.289380\n",
      "iter  90 value 1.283158\n",
      "iter 100 value 1.279714\n",
      "final  value 1.279714 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 24.606607\n",
      "iter  20 value 0.206549\n",
      "iter  30 value 0.000421\n",
      "final  value 0.000067 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 60.877093\n",
      "iter  20 value 59.478421\n",
      "iter  30 value 59.466944\n",
      "iter  30 value 59.466944\n",
      "iter  30 value 59.466944\n",
      "final  value 59.466944 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 24.687603\n",
      "iter  20 value 1.729995\n",
      "iter  30 value 1.540649\n",
      "iter  40 value 1.468005\n",
      "iter  50 value 1.403147\n",
      "iter  60 value 1.359195\n",
      "iter  70 value 1.323946\n",
      "iter  80 value 1.307510\n",
      "iter  90 value 1.297093\n",
      "iter 100 value 1.285403\n",
      "final  value 1.285403 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 19.719434\n",
      "iter  20 value 0.241529\n",
      "iter  30 value 0.000890\n",
      "final  value 0.000058 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 59.840572\n",
      "iter  20 value 58.285025\n",
      "iter  30 value 58.258446\n",
      "final  value 58.258442 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 19.808089\n",
      "iter  20 value 1.725815\n",
      "iter  30 value 1.539735\n",
      "iter  40 value 1.453378\n",
      "iter  50 value 1.414885\n",
      "iter  60 value 1.386563\n",
      "iter  70 value 1.367057\n",
      "iter  80 value 1.350670\n",
      "iter  90 value 1.344063\n",
      "iter 100 value 1.338518\n",
      "final  value 1.338518 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 24.878576\n",
      "iter  20 value 0.202472\n",
      "iter  30 value 0.000758\n",
      "final  value 0.000098 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 60.839242\n",
      "iter  20 value 58.972905\n",
      "iter  30 value 58.965822\n",
      "final  value 58.965820 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 24.970309\n",
      "iter  20 value 1.625660\n",
      "iter  30 value 1.436609\n",
      "iter  40 value 1.358465\n",
      "iter  50 value 1.306207\n",
      "iter  60 value 1.283254\n",
      "iter  70 value 1.266856\n",
      "iter  80 value 1.256459\n",
      "iter  90 value 1.249758\n",
      "iter 100 value 1.246347\n",
      "final  value 1.246347 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 17.750614\n",
      "iter  20 value 0.168646\n",
      "iter  30 value 0.000412\n",
      "final  value 0.000067 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 58.161101\n",
      "iter  20 value 56.263290\n",
      "iter  30 value 56.247416\n",
      "final  value 56.247411 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 17.847078\n",
      "iter  20 value 1.312743\n",
      "iter  30 value 1.226475\n",
      "iter  40 value 1.176917\n",
      "iter  50 value 1.139813\n",
      "iter  60 value 1.116317\n",
      "iter  70 value 1.096762\n",
      "iter  80 value 1.085872\n",
      "iter  90 value 1.079631\n",
      "iter 100 value 1.077303\n",
      "final  value 1.077303 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 24.891047\n",
      "iter  20 value 0.680871\n",
      "iter  30 value 0.001515\n",
      "final  value 0.000056 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 59.944147\n",
      "iter  20 value 58.661042\n",
      "iter  30 value 58.646758\n",
      "iter  30 value 58.646757\n",
      "iter  30 value 58.646757\n",
      "final  value 58.646757 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 24.993309\n",
      "iter  20 value 1.811747\n",
      "iter  30 value 1.547087\n",
      "iter  40 value 1.483612\n",
      "iter  50 value 1.441960\n",
      "iter  60 value 1.388633\n",
      "iter  70 value 1.359095\n",
      "iter  80 value 1.340806\n",
      "iter  90 value 1.332424\n",
      "iter 100 value 1.328892\n",
      "final  value 1.328892 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 24.595326\n",
      "iter  20 value 0.249939\n",
      "iter  30 value 0.002470\n",
      "final  value 0.000082 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 62.242032\n",
      "iter  20 value 60.440291\n",
      "iter  30 value 60.415503\n",
      "final  value 60.415499 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 24.673520\n",
      "iter  20 value 1.691161\n",
      "iter  30 value 1.523834\n",
      "iter  40 value 1.446974\n",
      "iter  50 value 1.396597\n",
      "iter  60 value 1.362451\n",
      "iter  70 value 1.345177\n",
      "iter  80 value 1.333159\n",
      "iter  90 value 1.327817\n",
      "iter 100 value 1.323243\n",
      "final  value 1.323243 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 24.843868\n",
      "iter  20 value 0.185542\n",
      "iter  30 value 0.003073\n",
      "final  value 0.000056 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 60.312406\n",
      "iter  20 value 58.260130\n",
      "iter  30 value 58.233106\n",
      "final  value 58.233102 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 24.931918\n",
      "iter  20 value 1.284258\n",
      "iter  30 value 1.182942\n",
      "iter  40 value 1.141548\n",
      "iter  50 value 1.116851\n",
      "iter  60 value 1.105062\n",
      "iter  70 value 1.099605\n",
      "iter  80 value 1.096248\n",
      "iter  90 value 1.092816\n",
      "iter 100 value 1.090800\n",
      "final  value 1.090800 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 25.934205\n",
      "iter  20 value 0.419181\n",
      "iter  30 value 0.000818\n",
      "final  value 0.000056 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 62.695423\n",
      "iter  20 value 61.218504\n",
      "iter  30 value 61.192318\n",
      "final  value 61.192311 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 26.032130\n",
      "iter  20 value 1.781536\n",
      "iter  30 value 1.592825\n",
      "iter  40 value 1.514112\n",
      "iter  50 value 1.472124\n",
      "iter  60 value 1.432956\n",
      "iter  70 value 1.408793\n",
      "iter  80 value 1.391105\n",
      "iter  90 value 1.382819\n",
      "iter 100 value 1.376739\n",
      "final  value 1.376739 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 24.784552\n",
      "iter  20 value 0.202609\n",
      "iter  30 value 0.002106\n",
      "final  value 0.000069 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 59.397092\n",
      "iter  20 value 57.901764\n",
      "iter  30 value 57.886890\n",
      "final  value 57.886889 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 24.867665\n",
      "iter  20 value 1.369917\n",
      "iter  30 value 1.237173\n",
      "iter  40 value 1.204071\n",
      "iter  50 value 1.176255\n",
      "iter  60 value 1.161142\n",
      "iter  70 value 1.150088\n",
      "iter  80 value 1.141617\n",
      "iter  90 value 1.135587\n",
      "iter 100 value 1.132275\n",
      "final  value 1.132275 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 25.750711\n",
      "iter  20 value 0.282013\n",
      "iter  30 value 0.001056\n",
      "final  value 0.000097 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 61.209189\n",
      "iter  20 value 59.865513\n",
      "iter  30 value 59.834427\n",
      "final  value 59.834422 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 25.845456\n",
      "iter  20 value 1.602104\n",
      "iter  30 value 1.424231\n",
      "iter  40 value 1.369767\n",
      "iter  50 value 1.336092\n",
      "iter  60 value 1.307517\n",
      "iter  70 value 1.291586\n",
      "iter  80 value 1.280754\n",
      "iter  90 value 1.274370\n",
      "iter 100 value 1.271425\n",
      "final  value 1.271425 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 236.587373 \n",
      "iter  10 value 27.645255\n",
      "iter  20 value 0.559942\n",
      "iter  30 value 0.002213\n",
      "final  value 0.000060 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 236.587373 \n",
      "iter  10 value 62.876936\n",
      "iter  20 value 61.320033\n",
      "iter  30 value 61.298115\n",
      "final  value 61.298113 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 236.587373 \n",
      "iter  10 value 27.747286\n",
      "iter  20 value 1.759823\n",
      "iter  30 value 1.519584\n",
      "iter  40 value 1.469158\n",
      "iter  50 value 1.441392\n",
      "iter  60 value 1.408287\n",
      "iter  70 value 1.385666\n",
      "iter  80 value 1.368219\n",
      "iter  90 value 1.361143\n",
      "iter 100 value 1.355489\n",
      "final  value 1.355489 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 18.668749\n",
      "iter  20 value 0.200464\n",
      "iter  30 value 0.000476\n",
      "final  value 0.000063 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 58.359841\n",
      "iter  20 value 57.259840\n",
      "iter  30 value 57.251959\n",
      "iter  30 value 57.251958\n",
      "iter  30 value 57.251958\n",
      "final  value 57.251958 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 18.775594\n",
      "iter  20 value 1.461784\n",
      "iter  30 value 1.322489\n",
      "iter  40 value 1.254806\n",
      "iter  50 value 1.215437\n",
      "iter  60 value 1.184530\n",
      "iter  70 value 1.159535\n",
      "iter  80 value 1.150578\n",
      "iter  90 value 1.144612\n",
      "iter 100 value 1.140734\n",
      "final  value 1.140734 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 25.042630\n",
      "iter  20 value 0.630360\n",
      "iter  30 value 0.001920\n",
      "final  value 0.000091 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 60.014464\n",
      "iter  20 value 58.418931\n",
      "iter  30 value 58.405715\n",
      "iter  30 value 58.405715\n",
      "iter  30 value 58.405715\n",
      "final  value 58.405715 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 25.139642\n",
      "iter  20 value 1.754954\n",
      "iter  30 value 1.518986\n",
      "iter  40 value 1.453135\n",
      "iter  50 value 1.404067\n",
      "iter  60 value 1.364962\n",
      "iter  70 value 1.336298\n",
      "iter  80 value 1.325411\n",
      "iter  90 value 1.319420\n",
      "iter 100 value 1.315556\n",
      "final  value 1.315556 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 30.957520\n",
      "iter  20 value 0.360651\n",
      "iter  30 value 0.004690\n",
      "final  value 0.000055 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 60.890640\n",
      "iter  20 value 59.304472\n",
      "iter  30 value 59.291178\n",
      "iter  30 value 59.291178\n",
      "iter  30 value 59.291178\n",
      "final  value 59.291178 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 31.054647\n",
      "iter  20 value 1.589067\n",
      "iter  30 value 1.446436\n",
      "iter  40 value 1.407584\n",
      "iter  50 value 1.358846\n",
      "iter  60 value 1.320175\n",
      "iter  70 value 1.298106\n",
      "iter  80 value 1.281960\n",
      "iter  90 value 1.275684\n",
      "iter 100 value 1.270082\n",
      "final  value 1.270082 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 20.813661\n",
      "iter  20 value 0.241373\n",
      "iter  30 value 0.001455\n",
      "final  value 0.000098 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 61.705437\n",
      "iter  20 value 60.043165\n",
      "iter  30 value 60.031485\n",
      "final  value 60.031483 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 20.915475\n",
      "iter  20 value 1.591004\n",
      "iter  30 value 1.419356\n",
      "iter  40 value 1.357744\n",
      "iter  50 value 1.320046\n",
      "iter  60 value 1.295900\n",
      "iter  70 value 1.272305\n",
      "iter  80 value 1.260945\n",
      "iter  90 value 1.256193\n",
      "iter 100 value 1.253636\n",
      "final  value 1.253636 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 20.736354\n",
      "iter  20 value 0.323931\n",
      "iter  30 value 0.001011\n",
      "final  value 0.000067 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 62.333701\n",
      "iter  20 value 59.973861\n",
      "iter  30 value 59.941422\n",
      "final  value 59.941416 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 233.368497 \n",
      "iter  10 value 20.826764\n",
      "iter  20 value 1.719838\n",
      "iter  30 value 1.558815\n",
      "iter  40 value 1.484191\n",
      "iter  50 value 1.428570\n",
      "iter  60 value 1.393366\n",
      "iter  70 value 1.369007\n",
      "iter  80 value 1.362524\n",
      "iter  90 value 1.356240\n",
      "iter 100 value 1.351660\n",
      "final  value 1.351660 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 236.587373 \n",
      "iter  10 value 23.238039\n",
      "iter  20 value 0.532609\n",
      "iter  30 value 0.001070\n",
      "final  value 0.000075 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 236.587373 \n",
      "iter  10 value 62.097628\n",
      "iter  20 value 60.608853\n",
      "iter  30 value 60.595334\n",
      "final  value 60.595333 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 236.587373 \n",
      "iter  10 value 23.333000\n",
      "iter  20 value 1.713328\n",
      "iter  30 value 1.522022\n",
      "iter  40 value 1.463580\n",
      "iter  50 value 1.414307\n",
      "iter  60 value 1.371342\n",
      "iter  70 value 1.347403\n",
      "iter  80 value 1.331980\n",
      "iter  90 value 1.322421\n",
      "iter 100 value 1.315530\n",
      "final  value 1.315530 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 24.623310\n",
      "iter  20 value 0.213282\n",
      "iter  30 value 0.008213\n",
      "final  value 0.000086 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 59.083287\n",
      "iter  20 value 57.426486\n",
      "iter  30 value 57.399937\n",
      "final  value 57.399933 \n",
      "converged\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 234.977935 \n",
      "iter  10 value 24.712140\n",
      "iter  20 value 1.524227\n",
      "iter  30 value 1.370125\n",
      "iter  40 value 1.305760\n",
      "iter  50 value 1.271143\n",
      "iter  60 value 1.244110\n",
      "iter  70 value 1.231427\n",
      "iter  80 value 1.218551\n",
      "iter  90 value 1.214480\n",
      "iter 100 value 1.211765\n",
      "final  value 1.211765 \n",
      "stopped after 100 iterations\n",
      "# weights:  85 (64 variable)\n",
      "initial  value 260.728942 \n",
      "iter  10 value 43.044713\n",
      "iter  20 value 1.871801\n",
      "iter  30 value 1.654110\n",
      "iter  40 value 1.583398\n",
      "iter  50 value 1.516625\n",
      "iter  60 value 1.472608\n",
      "iter  70 value 1.448958\n",
      "iter  80 value 1.436617\n",
      "iter  90 value 1.427280\n",
      "iter 100 value 1.417552\n",
      "final  value 1.417552 \n",
      "stopped after 100 iterations\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Penalized Multinomial Regression \n",
       "\n",
       "162 samples\n",
       "  6 predictor\n",
       "  5 classes: 'drugA', 'drugB', 'drugC', 'drugX', 'drugY' \n",
       "\n",
       "Pre-processing: re-scaling to [0, 1] (15) \n",
       "Resampling: Cross-Validated (10 fold, repeated 3 times) \n",
       "Summary of sample sizes: 146, 146, 145, 146, 147, 145, ... \n",
       "Resampling results across tuning parameters:\n",
       "\n",
       "  decay  Accuracy   Kappa    \n",
       "  0e+00  0.8860049  0.8430152\n",
       "  1e-04  0.9239869  0.8941879\n",
       "  1e-01  0.8642892  0.8080702\n",
       "\n",
       "Accuracy was used to select the optimal model using the largest value.\n",
       "The final value used for the model was decay = 1e-04."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multinomFit <- train(Drug ~ ., data = dc_training, method = \"multinom\", trControl = ctrl, preProcess = c(\"range\"))\n",
    "multinomFit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Confusion Matrix and Statistics\n",
       "\n",
       "          Reference\n",
       "Prediction drugA drugB drugC drugX drugY\n",
       "     drugA     4     0     0     0     0\n",
       "     drugB     1     4     0     0     0\n",
       "     drugC     0     0     8     0     3\n",
       "     drugX     0     0     0    12     0\n",
       "     drugY     0     0     0     1    19\n",
       "\n",
       "Overall Statistics\n",
       "                                         \n",
       "               Accuracy : 0.9038         \n",
       "                 95% CI : (0.7897, 0.968)\n",
       "    No Information Rate : 0.4231         \n",
       "    P-Value [Acc > NIR] : 4.966e-13      \n",
       "                                         \n",
       "                  Kappa : 0.8687         \n",
       " Mcnemar's Test P-Value : NA             \n",
       "\n",
       "Statistics by Class:\n",
       "\n",
       "                     Class: drugA Class: drugB Class: drugC Class: drugX\n",
       "Sensitivity               0.80000      1.00000       1.0000       0.9231\n",
       "Specificity               1.00000      0.97917       0.9318       1.0000\n",
       "Pos Pred Value            1.00000      0.80000       0.7273       1.0000\n",
       "Neg Pred Value            0.97917      1.00000       1.0000       0.9750\n",
       "Prevalence                0.09615      0.07692       0.1538       0.2500\n",
       "Detection Rate            0.07692      0.07692       0.1538       0.2308\n",
       "Detection Prevalence      0.07692      0.09615       0.2115       0.2308\n",
       "Balanced Accuracy         0.90000      0.98958       0.9659       0.9615\n",
       "                     Class: drugY\n",
       "Sensitivity                0.8636\n",
       "Specificity                0.9667\n",
       "Pos Pred Value             0.9500\n",
       "Neg Pred Value             0.9062\n",
       "Prevalence                 0.4231\n",
       "Detection Rate             0.3654\n",
       "Detection Prevalence       0.3846\n",
       "Balanced Accuracy          0.9152"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "multinomPredict <- predict(multinomFit, newdata = dc_testing)\n",
    "confusionMatrix(multinomPredict, dc_testing$Drug)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.903846153846154"
      ],
      "text/latex": [
       "0.903846153846154"
      ],
      "text/markdown": [
       "0.903846153846154"
      ],
      "text/plain": [
       "[1] 0.9038462"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dc_multinom_accuracy <- mean(multinomPredict == dc_testing$Drug)\n",
    "dc_multinom_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resumen de \"accuracy\" en \"discretized\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAABHVBMVEUAAAABAQEDAwMFBQUH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\ngg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "R display func"
    }
   ],
   "source": [
    "foo <- matrix(c(0.557, 0.885, 0.654, 0.904, 0.885, 0.903), nrow=1, ncol=6)\n",
    "colnames(foo) <- c(\"kNN\", \"Rand. Frst\", \"N. Bayes\", \"SVM\", \"M. Percp.\", \"Logistic\")\n",
    "barplot(foo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Promedio de efectividad en cada conjunto de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se calcula el promedio de efectividad que tuvo el conjunto de datos en distintas configuraciones de preprocesado. donde el procesado que se aplic a cada conjunto de datos es el siguiente:\n",
    "\n",
    "**cleaned**: se removieron valores atipicos\n",
    "\n",
    "**smoothed:** *cleaned* + suavizado de datos\n",
    "\n",
    "**discretized:** *cleaned* + *smoothed* + se discretizaron los datos numericos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAAAwFBMVEUAAAABAQENDQ0REREW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\nAaDRyS0YPg2xAAAAAElFTkSuQmCC",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "R display func"
    }
   ],
   "source": [
    "foo <- matrix(c(0.86, 0.84, 0.557), nrow=1, ncol=3)\n",
    "colnames(foo) <- c(\"cleaned\", \"smoothed\", \"discretized\")\n",
    "barplot(foo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mejor configuracion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se comprob experimentalmente que la mejor configuracion de preprocesado conciste unicamente en eliminar registros atipicos, asi mismo el algoritmo que arroja mayor precision pertenece al conjunto de datos mencionado anteriormente, siendo este el de regresion logistica con una efectividad del 97%.\n",
    "\n",
    "Tambien es interesante notar la poca efectividad de los metodos bayesianos en todas las configuraciones, y su relativa mejora de precision cuando los datos han sido limpiados, suavizados y discretizados. Aun asi los metodos bayesianos no logran una efectividad mayor al 65% en ninguna configuracin."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision promedio de cada metodo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0gAAANICAMAAADKOT/pAAABJlBMVEUAAAABAQECAgIEBAQF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\ngJAgICQICAkC/w+TM3cVmUL0FgAAAABJRU5ErkJggg==",
      "text/plain": [
       "plot without title"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "R display func"
    }
   ],
   "source": [
    "foo <- matrix(c(0.69, 0.89, 0.58, 0.93, 0.92, 0.95), nrow=1, ncol=6)\n",
    "colnames(foo) <- c(\"kNN\", \"Rand. Frst\", \"N. Bayes\", \"SVM\", \"M. Percp.\", \"Logistic\")\n",
    "barplot(foo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De esta manera se concluye que los mejores resultados los arroj la regresion logistica, seguida de cerca por la SVM y las redes neuronales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
